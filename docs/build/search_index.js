var documenterSearchIndex = {"docs":
[{"location":"#RiemannianML-Documentation-1","page":"RiemannianML Documentation","title":"RiemannianML Documentation","text":"","category":"section"},{"location":"#Requirements-1","page":"RiemannianML Documentation","title":"Requirements","text":"","category":"section"},{"location":"#","page":"RiemannianML Documentation","title":"RiemannianML Documentation","text":"Julia version ≥ 1.0.3 Python ScikitLearn ","category":"page"},{"location":"#","page":"RiemannianML Documentation","title":"RiemannianML Documentation","text":"This package makes use of :","category":"page"},{"location":"#","page":"RiemannianML Documentation","title":"RiemannianML Documentation","text":"PosDefManifold.jl\nScikitLearn.Crossvalidation","category":"page"},{"location":"#Installation-1","page":"RiemannianML Documentation","title":"Installation","text":"","category":"section"},{"location":"#","page":"RiemannianML Documentation","title":"RiemannianML Documentation","text":"Execute the following command in Julia's REPL:","category":"page"},{"location":"#","page":"RiemannianML Documentation","title":"RiemannianML Documentation","text":"]add RiemannianML","category":"page"},{"location":"#","page":"RiemannianML Documentation","title":"RiemannianML Documentation","text":"To obtain the latest development version execute instead","category":"page"},{"location":"#","page":"RiemannianML Documentation","title":"RiemannianML Documentation","text":"]add RiemannianML#master","category":"page"},{"location":"#Overview-1","page":"RiemannianML Documentation","title":"Overview","text":"","category":"section"},{"location":"#","page":"RiemannianML Documentation","title":"RiemannianML Documentation","text":"Riemannian geometry studies smooth manifolds, multi-dimensional curved spaces with peculiar geometries endowed with non-Euclidean metrics. In these spaces Riemannian geometry allows the definition of angles, geodesics (shortest path between two points), distances between points, centers of mass of several points, etc.","category":"page"},{"location":"#","page":"RiemannianML Documentation","title":"RiemannianML Documentation","text":"In several fields of research such as computer vision and brain-computer interface, treating data in the P manifold has allowed the introduction of machine learning approaches with remarkable characteristics, such as simplicity of use, excellent classification accuracy, as demonstrated by the winning score obtained in six international data classification competitions, and the ability to operate transfer learning (Congedo et al., 2017a, Brachant et al.))","category":"page"},{"location":"#","page":"RiemannianML Documentation","title":"RiemannianML Documentation","text":"In this package we are concerned with making use of Riemannian Geometry in classification of data in the manifold of Positive Definite Matrices. This can be done in two ways, either in the Positive Definite Manifold or in the Tangent Space of mapped data. ","category":"page"},{"location":"#","page":"RiemannianML Documentation","title":"RiemannianML Documentation","text":"Positive Definite Manifold : Here we can use different distance metrics to compute the distances between the points represented by positive definite matrices. Using thsi we apply MDM(Minimum Distance to Mean) criteria for classifying test data in the positive defininite manifold.\nTangent Space : Here the data points from the manifold of positive definite matrices are mapped to their corresponding points in the tangent space of the mean point of the set. Since the manifold of positive definite matrices forms a Riemannian manifold, so this brings us the opportunity to implement all the machine learning algorithms from scikit-learn. The reason being that the tangent space behaves like an Eucledian space, so it opens the way to all the general machine learning algorithms implemented in sckit-learn as they assume the metric apace to be Eucledian. The mapping is done using a specific function logmap from the PosDefManifold.","category":"page"},{"location":"#","page":"RiemannianML Documentation","title":"RiemannianML Documentation","text":"The important point to be marked is that our points are not vectors but covariance matrices in the Positive Definite Manifold. This is unlike what we have in traditional Machine Learning i.e. training examples as vectors. So, to apply traditional machine learning algorithms we first perform this mapping into the tangent space.","category":"page"},{"location":"#","page":"RiemannianML Documentation","title":"RiemannianML Documentation","text":"(Image: Figure 1) Figure 1. Schematic representation of the entire process. The raw data which are first converted into their covariance matrices. Following that we have the two choices i.e. directly do classification in the manifold or map the points in the tangent space and apply Machine Learning models.","category":"page"},{"location":"#","page":"RiemannianML Documentation","title":"RiemannianML Documentation","text":"This package makes extensive use of the functions from the package PostDefManifold ","category":"page"},{"location":"#","page":"RiemannianML Documentation","title":"RiemannianML Documentation","text":"For a formal introduction to the P manifold the reader is referred to the monography written by R. Bhatia (2007) Positive Definite Matrices. Princeton University press.","category":"page"},{"location":"#","page":"RiemannianML Documentation","title":"RiemannianML Documentation","text":"For an introduction to Riemannian geometry and an overview of mathematical tools implemented in this package, see Intro to Riemannian Geometry in the documentation of PostDefManifold.","category":"page"},{"location":"#","page":"RiemannianML Documentation","title":"RiemannianML Documentation","text":"For starting using this package, browse the code units listed here below and execute the many example section you will find therein.","category":"page"},{"location":"#Code-units-1","page":"RiemannianML Documentation","title":"Code units","text":"","category":"section"},{"location":"#","page":"RiemannianML Documentation","title":"RiemannianML Documentation","text":"RiemannianML includes seven code units (.jl files):","category":"page"},{"location":"#","page":"RiemannianML Documentation","title":"RiemannianML Documentation","text":"Unit Description\nMainModule (RiemannianML.jl) Main module, constants, types, aliases, tips & tricks\nknn.jl Unit implementing Kneighbhor Classification\nlogisticRegression.jl Unit implementing LogisticRegression and LogisticRegressionCV\nSVM.jl Unit implementing LinearSVC and SVC\nmdm.jl Unit implementing MDM( Minimum Distance to Mean) classification\ntrain_test.jl Unit implementing the tranformation function. Along with overwriting fit!, predict and crossvalscore from the ScikitLearn.jl package\nexample.jl Unit containing examples for understanding and execution","category":"page"},{"location":"#-1","page":"RiemannianML Documentation","title":"🎓","text":"","category":"section"},{"location":"#","page":"RiemannianML Documentation","title":"RiemannianML Documentation","text":"References","category":"page"},{"location":"#","page":"RiemannianML Documentation","title":"RiemannianML Documentation","text":"M. Congedo, A. Barachant, R. Bhatia R (2017a) Riemannian Geometry for EEG-based Brain-Computer Interfaces; a Primer and a Review, Brain-Computer Interfaces, 4(3), 155-174.","category":"page"},{"location":"#","page":"RiemannianML Documentation","title":"RiemannianML Documentation","text":"Alexandre Barachant 1 Stéphane Bonnet 1 Marco Congedo 2 Christian Jutten 2 Multiclass Brain-Computer Interface Classification by Riemannian Geometry","category":"page"},{"location":"#","page":"RiemannianML Documentation","title":"RiemannianML Documentation","text":"R. Bhatia (2007) Positive Definite Matrices. Princeton University press.","category":"page"},{"location":"#","page":"RiemannianML Documentation","title":"RiemannianML Documentation","text":"A. Barachant, S. Bonnet, M. Congedo, C. Jutten (2012) Multi-class Brain Computer Interface Classification by Riemannian Geometry, IEEE Transactions on Biomedical Engineering, 59(4), 920-928.","category":"page"},{"location":"#","page":"RiemannianML Documentation","title":"RiemannianML Documentation","text":"A. Barachant, S. Bonnet, M. Congedo, C. Jutten (2013) Classification of covariance matrices using a Riemannian-based kernel for BCI applications, Neurocomputing, 112, 172-178.","category":"page"},{"location":"#","page":"RiemannianML Documentation","title":"RiemannianML Documentation","text":"M. Congedo, A. Barachant, R. Bhatia R (2017a) Riemannian Geometry for EEG-based Brain-Computer Interfaces; a Primer and a Review, Brain-Computer Interfaces, 4(3), 155-174.","category":"page"},{"location":"#","page":"RiemannianML Documentation","title":"RiemannianML Documentation","text":"M. Congedo, A. Barachant, E. Kharati Koopaei (2017b) Fixed Point Algorithms for Estimating Power Means of Positive Definite Matrices, IEEE Transactions on Signal Processing, 65(9), 2211-2220.","category":"page"},{"location":"#","page":"RiemannianML Documentation","title":"RiemannianML Documentation","text":"Or one may directly look into the Intro to Riemannian Geometry section of the PostDefManifold documentation and quench all their doubts. ","category":"page"},{"location":"#Contents-1","page":"RiemannianML Documentation","title":"Contents","text":"","category":"section"},{"location":"#","page":"RiemannianML Documentation","title":"RiemannianML Documentation","text":"Pages = [       \"index.md\",\r\n                \"MainModule.md\",\r\n                \"knn.md\",\r\n                \"logisticRegression.md\",\r\n                \"SVM.md\",\r\n                \"mdm.md\",\r\n\t\t\"train_test.md\",\r\n\t\t\"example.md\"]\r\nDepth = 1","category":"page"},{"location":"#Index-1","page":"RiemannianML Documentation","title":"Index","text":"","category":"section"},{"location":"#","page":"RiemannianML Documentation","title":"RiemannianML Documentation","text":"","category":"page"},{"location":"MainModule/#MainModule-(RiemannianML.jl)-1","page":"MainModule (RiemannianML.jl)","title":"MainModule (RiemannianML.jl)","text":"","category":"section"},{"location":"MainModule/#","page":"MainModule (RiemannianML.jl)","title":"MainModule (RiemannianML.jl)","text":"This is the main unit containing the RiemannianML module.","category":"page"},{"location":"MainModule/#","page":"MainModule (RiemannianML.jl)","title":"MainModule (RiemannianML.jl)","text":"It uses the following standard Julia packages:","category":"page"},{"location":"MainModule/#","page":"MainModule (RiemannianML.jl)","title":"MainModule (RiemannianML.jl)","text":"using\nLinearAlgebra\nStatistics\nScikitLearn\nRandom","category":"page"},{"location":"MainModule/#","page":"MainModule (RiemannianML.jl)","title":"MainModule (RiemannianML.jl)","text":"The main module does not contains functions, but it declares all imports and exports of Julia functions used or created in all the units.","category":"page"},{"location":"knn/#knn.jl-1","page":"knn.jl","title":"knn.jl","text":"","category":"section"},{"location":"knn/#","page":"knn.jl","title":"knn.jl","text":"This unit implements the KNeighborsClassifier model for the data points in the Riemannian manifold of Symmetric Positive Definite (SPD). The structure below is similar to a class of KNeighborsClassifier we have in ScikitLearn in Python. For further information on kneighborClf, one may refer to the scikit-learn documentation of KNeighborsClassifier.","category":"page"},{"location":"knn/#","page":"knn.jl","title":"knn.jl","text":"An object of scikit-learn KNeighborsClassifier is automatically created by the constructor as soon as an object of this struct is created by the user. ","category":"page"},{"location":"knn/#kneighborClf-structure-1","page":"knn.jl","title":"kneighborClf  structure","text":"","category":"section"},{"location":"knn/#","page":"knn.jl","title":"knn.jl","text":"kneighborClf","category":"page"},{"location":"knn/#RiemannianML.kneighborClf","page":"knn.jl","title":"RiemannianML.kneighborClf","text":"Structure for implementing KNeighborsClassifier. The clf attribute is the python object of KNeighborsClassifier that is constructed by the constructor function of this structure as soon as a kneighborClf instance is created by the user.\n\nParameters :\n\nn_neighbors :: Int64\nweights:: String\nalgorithm:: String\nleaf_size:: Int64\np:: Int64\nmetric:: String\nmetric_params :: Dict\nn_jobs :: Int\n\nOne may refer to the ScikitLearn documentation of KNeighborsClassifier for the better understanding of these parameters with respect to kneighborClf.\n\n## Example\nusing RiemannianML\nmodel = kneighborClf(n_neighbors=3)\n\n\n\n\n\n","category":"type"},{"location":"logisticRegression/#logisticRegression.jl-1","page":"logisticRegression.jl","title":"logisticRegression.jl","text":"","category":"section"},{"location":"logisticRegression/#","page":"logisticRegression.jl","title":"logisticRegression.jl","text":"This unit implements the LogisticRegression models for the data points in the Riemannian manifold of Symmetric Positive Definite (SPD). The structures below are similar to classes of LogisticRegression and LogisticRegressionCV we have in ScikitLearn in Python. This Module incorporates two models of LogisticRegression :     - LogisticReg -   Simple logisticRegression in which the value of the penalty                        coefficient alpha is set manually by the user. The user is                        responsible for choosing a specific value of alpha.     - LogisticRegCV - LogisticRegression with cross validation, to find the best                        suit for alpha. The user can provide a range of alpha and the                        algorithm finds the best suited value of alpha out of those.","category":"page"},{"location":"logisticRegression/#","page":"logisticRegression.jl","title":"logisticRegression.jl","text":"For further information on LogisticReg and LogisticRegCV one may refer to the scikit-learn    documentations of LogisticRegression     and LogisticRegressionCV     respectively.","category":"page"},{"location":"logisticRegression/#","page":"logisticRegression.jl","title":"logisticRegression.jl","text":"An object of scikit-learn LogisticRegression or LogisticRegressionCV is automatically created by the constructor as soon as an instance of their structs is created by the user.","category":"page"},{"location":"logisticRegression/#LogisticReg-structure-1","page":"logisticRegression.jl","title":"LogisticReg structure","text":"","category":"section"},{"location":"logisticRegression/#","page":"logisticRegression.jl","title":"logisticRegression.jl","text":"LogisticReg","category":"page"},{"location":"logisticRegression/#RiemannianML.LogisticReg","page":"logisticRegression.jl","title":"RiemannianML.LogisticReg","text":"Structure for implementing LogisticRegression, with all the arguments as attributes to this structure. The 'clf' attribute is the python object of LogisticRegression that is constructed by the constructor function of this structure as soon as a LogisticReg instance is created by the user.\n\nParameters :\n\npenalty:: String\ndual :: Bool\ntol :: Float64\nC :: Float64\nfit_intercept :: Bool\nintercept_scaling :: Float64\nclass_weight :: Dict\nrandom_state :: Int\nsolver :: String\nmax_iter :: Int64\nmulti_class :: String\nverbose :: Int64\nwarm_start :: Bool\nn_jobs :: Int\nl1_ratio :: Float64\n\nOne may refer to the ScikitLearn documentation of LogisticRegression for the better understanding of these parameters with respect to LogisticRegression.\n\n## Example\nusing RiemannianML\nmodel = LogisticReg()\n\n\n\n\n\n","category":"type"},{"location":"logisticRegression/#LogisticRegCV-structure-1","page":"logisticRegression.jl","title":"LogisticRegCV structure","text":"","category":"section"},{"location":"logisticRegression/#","page":"logisticRegression.jl","title":"logisticRegression.jl","text":"LogisticRegCV","category":"page"},{"location":"logisticRegression/#RiemannianML.LogisticRegCV","page":"logisticRegression.jl","title":"RiemannianML.LogisticRegCV","text":"Structure for implementing LogisticRegressionCV, with all the arguments as attributes to this structure. The 'clf' attribute is the python object of LogisticRegressionCV that is constructed by the constructor function of this structure as soon as a LogisticRegCV instance is created by the user.\n\nParameters :\n\nCs :: Int[] or Float64[]\nfit_intercept :: Bool\ncv :: Int\ndual :: Bool\npenalty :: String\nscoring :: String\nsolver :: String\ntol :: Float64\nmax_iter :: Int\nclass_weight :: Dict\nn_jobs :: Int\nverbose :: Int\nrefit :: Bool\nintercept_scaling :: Float64\nmulti_class :: String\nrandom_state :: Int\nl1_ratios :: Float64\n\nOne should refer to the ScikitLearn documentation of LogisticRegressionCV for the better understanding of these parameters with respect to LogisticRegressionCV.\n\n## Example\nusing RiemannianML\nmodel = LogisticRegCV()\n\n\n\n\n\n","category":"type"},{"location":"SVM/#SVM.jl-1","page":"SVM.jl","title":"SVM.jl","text":"","category":"section"},{"location":"SVM/#","page":"SVM.jl","title":"SVM.jl","text":"This unit implements the Support Vector models for the data points in the manifold of positive definite matrices. The structures below are similar to classes of LinearSVC and SVC we have in ScikitLearn in Python. This Module incorporates two models of Support Vector Machine type :     - LinearSVM     - SVM","category":"page"},{"location":"SVM/#","page":"SVM.jl","title":"SVM.jl","text":"For further information on LinearSVM and SVM one may refer to the scikit-learn documentations of LinearSVC and SVC respectively.","category":"page"},{"location":"SVM/#","page":"SVM.jl","title":"SVM.jl","text":"An object of scikit-learn LinearSVC or SVC is automatically created by the constructor as soon as an instance of their structs is created by the user.","category":"page"},{"location":"SVM/#LinearSVM-structure-1","page":"SVM.jl","title":"LinearSVM  structure","text":"","category":"section"},{"location":"SVM/#","page":"SVM.jl","title":"SVM.jl","text":"LinearSVM","category":"page"},{"location":"SVM/#RiemannianML.LinearSVM","page":"SVM.jl","title":"RiemannianML.LinearSVM","text":"Structure for implementing LinearSVC, with all the arguments as attributes to this structure. The 'clf' attribute is the python object of LinearSVC that is constructed by the constructor function of this structure as soon as a LinearSVM instance is created by the user.\n\nParameters :\n\npenalty:: String\nloss :: String\ndual :: Bool\ntol :: Float64\nC :: Float64\nmulti_class :: String\nfit_intercept :: Bool\nintercept_scaling :: Float64\nclass_weight :: Dict\nverbose :: Int64\nrandom_state :: Int\nmax_iter :: Int64\n\nOne should refer to the ScikitLearn documentation of LinearSVC for the better understanding of these parameters with respect to Linear Support Vector Machine.\n\n## Example\nusing RiemannianML\nmodel = LinearSVM()\n\n\n\n\n\n","category":"type"},{"location":"SVM/#SVM-structure-1","page":"SVM.jl","title":"SVM  structure","text":"","category":"section"},{"location":"SVM/#","page":"SVM.jl","title":"SVM.jl","text":"SVM","category":"page"},{"location":"SVM/#RiemannianML.SVM","page":"SVM.jl","title":"RiemannianML.SVM","text":"Structure for implementing SVC, with all the arguments as attributes to this structure. The 'clf' attribute is the python object of SVC that is constructed by the constructor function of this structure as soon as a SVM instance is created by the user.\n\nParameters :\n\npenalty:: String\nloss :: String\ndual :: Bool\ntol :: Float64\nC :: Float64\nmulti_class :: String\nfit_intercept :: Bool\nintercept_scaling :: Float64\nclass_weight :: Dict\nverbose :: Int64\nrandom_state :: Int\nmax_iter :: Int64\n\nOne should refer to the ScikitLearn documentation of SVC for the better understanding of these parameters with respect to Support Vector Machine.\n\n## Example\nusing RiemannianML\nmodel = SVM()\n\n\n\n\n\n","category":"type"},{"location":"train_test/#train_test.jl-1","page":"train_test.jl","title":"train_test.jl","text":"","category":"section"},{"location":"train_test/#","page":"train_test.jl","title":"train_test.jl","text":"This unit implements the tranformation function. Along with that this unit overwrites the fit!, predict and crossvalscore from the ScikitLearn.jl package. This enables us to use the same functions even for data in the manifold of positive definite matrices.","category":"page"},{"location":"train_test/#","page":"train_test.jl","title":"train_test.jl","text":"It imports the required machine learning models from scikit-learn python using PyCall. This unit includes the following functions :","category":"page"},{"location":"train_test/#","page":"train_test.jl","title":"train_test.jl","text":"Function Description\nlogMap internal function that projects the points in the SPD manifold into the tangent space\nfit! fits the model for the given training set\npredict makes prediction for the points in the test set\ncross_val_score evaluates the cross-validation score of the estimator or model","category":"page"},{"location":"train_test/#logMap-1","page":"train_test.jl","title":"logMap","text":"","category":"section"},{"location":"train_test/#","page":"train_test.jl","title":"train_test.jl","text":"logMap","category":"page"},{"location":"train_test/#PosDefManifold.logMap","page":"train_test.jl","title":"PosDefManifold.logMap","text":"Given a ℍVector( Vector of Hermitian matrices), weights(optional), returns a vector containing the mapping of these matrices in the tangent space of the data set mean. Vectorization is done along with the mapping. The relation employed for mapping is the following logarithmic relation:         (G½ * log(ℍ(G½ * 𝐏i * G½)) * G½) where G is the data set mean and 𝐏[i] the set of points to be mapped. For the better understanding of this mapping, one may refer to the logMap function in PostDefManifold.\n\nArguments:\n\n𝐏::ℍVector              :- Vector of Hermitian matrices or simply a HermitianVector.                                    The vector of points in the Symmetric Positive Definite                                    manifold to be transformed into the the tangent space.\n\nThe following parameters are needed for mean computation only:\n\nw::Vector(optional):- Vector containing weights corresponding to every point                                    in 𝐏.\n✓w = true(optional)     :- Boolean to determine whether to calculate weighted mean                                    or just take w = []. It also checks if the weights                                    sum up to 1. If not does normalization.\n⏩ = false (optional)    :- Boolean to allow threading or not.\n\nReturns:\n\nVec :: Array{Float64, 2} :-Vector of all the points in the training set.\n\n\n\n\n\n","category":"function"},{"location":"train_test/#fit!-1","page":"train_test.jl","title":"fit!","text":"","category":"section"},{"location":"train_test/#","page":"train_test.jl","title":"train_test.jl","text":"fit!","category":"page"},{"location":"train_test/#ScikitLearnBase.fit!","page":"train_test.jl","title":"ScikitLearnBase.fit!","text":"Given a model, ℍVector(training set), y(labels), weights(optional), checks the type of model and then fits the model to the given training set. This function is an overwriting of the default fit! function available in the ScikitLearn.jl package. The function also prints the average regular score for all models except mdm.\n\nArguments :\n\nmodel::RiemannianML object:- Classifier model instance eg. kneighbhorClf(),                                  LogisticReg() or others. The model which is to                                  be trained or to which the given data is to be fit.                                  The instance should be created before calling fit! to                                  train it.\n𝐗::ℍVector               :- Vector of Hermitian matrices or simply a HermitianVector.                                 The vector of points in the training set consisting of                                 Symmetric Positive Definite manifold matrices.\ny :: Int[]               :- Vector of intrger labels corresponding to each sample in the                                 training set.\nw::Vector(optional) :- Vector containing weights corresponding to every point                                 in 𝐗. Only for mdm models.\n✓w = true(optional)      :- Boolean to determine whether to calculate weighted mean                                 or just take w = [].It also checks if the weights                                 sum up to 1. If not does normalization.\n\nReturns: (A value is returned only in case the model is an mdm object)\n\nclass_means              :- List of means corresponding to all the classes for the                                   given training set.\n\n## Example\nmodel1 = kneighborClf(n_neighbors=3)\nmodel2 = LogisticReg()\nmodel3 = MDM(Fisher)\n𝐗 = *load data...*\ny = *load labels...*\nfit!(model1, 𝐗,y)\nfit!(model2, 𝐗,y)\nfit!(model3, 𝐗,y)\n\n\n\n\n\n","category":"function"},{"location":"train_test/#predict-1","page":"train_test.jl","title":"predict","text":"","category":"section"},{"location":"train_test/#","page":"train_test.jl","title":"train_test.jl","text":"predict","category":"page"},{"location":"train_test/#ScikitLearnBase.predict","page":"train_test.jl","title":"ScikitLearnBase.predict","text":"Given a trained model and the sample set, gives the predicted class for the data points in the sample set. This function is an overwriting of the default predict function available in the ScikitLearn.jl package.\n\nArguments :\n\nmodel::RiemannianML object:- Classifier model instance eg. kneighbhorClf(),                                      LogisticReg() or others. The model which is already                                      been trained according to a training set can only be                                      used as an argument here. The instance should be                                      created and fit before calling predict on it.\nsamp::ℍVector             :- The vector of Hermitian matrices or points in                                      the positive definite manifold for which the                                      prediction is to be made using the model(argument 1)                                      already been trained specially for it.\n\nReturns :\n\nPredicted classes        :- The List of the predicted classes for the given                                   sample set.\n\n## Example\n# following the above code for fit!\npredict(model1, 𝐗)\npredict(model2, 𝐗)\npredict(model3, 𝐗)\n\n\n\n\n\n","category":"function"},{"location":"train_test/#cross*val*score-1","page":"train_test.jl","title":"crossvalscore","text":"","category":"section"},{"location":"train_test/#","page":"train_test.jl","title":"train_test.jl","text":"cross_val_score","category":"page"},{"location":"train_test/#ScikitLearn.Skcore.cross_val_score","page":"train_test.jl","title":"ScikitLearn.Skcore.cross_val_score","text":"Given a model, ℍVector(training set), y(labels), cv(number of cross-validations) returns the the list containing the score for each cross-validation iteration. This function is an overwriting of the default crossvalscore function available in the ScikitLearn.jl package.\n\nArguments :\n\nmodel::RiemannianML object:- Classifier model instance eg. kneighbhorClf(),                                      LogisticReg() or others i,e. the model whose                                      evaluation using cross-validation is to be done.\n𝐗::ℍVector                :- Vector of Hermitian matrices or simply a HermitianVector.                                      The vector of points in the training set consisting of                                      Symmetric Positive Definite manifold matrices. The                                      training set on the basis of which the evaluation                                      of the given model is to be done.\ny :: Int[]                :- Vector of intrger labels corresponding to each sample in the                                      training set.\ncv :: Int(optional)       :- The number of cross-validation desired by the user.                                      The default value is set to 5.\n\nThese arguments are only applicable if the model is mdm type :\n\nscoring :: String      :- If Balanced Accuracy is requied or the Regular Accuracy.                                   The default is set to Balanced Accuracy.\ncnfmat :: Bool         :- Boolean to notify if the user wants the confusion matrix also.                              Default is set to false.\n\nReturns :\n\ncross_val score          :- The list containing the score for each cross_val                                   iteration.\n\n   ## Example\n   model1 = kneighborClf(n_neighbors=3)\n   model2 = LogisticReg()\n   model3 = MDM(Fisher)\n   𝐗 = *load data...*\n   y = *load labels...*\n   println(cross_val_score(model1,𝐗,y))\n   println(cross_val_score(model2,𝐗,y))\n   cross_val_score(model5, 𝐗,y)\n\n\n\n\n\n","category":"function"},{"location":"mdm/#mdm.jl-1","page":"mdm.jl","title":"mdm.jl","text":"","category":"section"},{"location":"mdm/#","page":"mdm.jl","title":"mdm.jl","text":"This unit implemets the MDM (Minimum Distance to Mean) classifier for the manifold of positive definite matrices. Similarly to what is done in ScikitLearn in Python, a type is created (struct in Julia) of the desired specifications.  It also implements cross validation algorithm for MDM( Minimum Distance to Mean) classifier again similar to ScikitLearn in Python, one can have a better evaluation of the classifier by using cross validation. Module incorporates supporting functions :   find_dist,    predict_mdm,    predict_prob,   indCV.","category":"page"},{"location":"mdm/#","page":"mdm.jl","title":"mdm.jl","text":"It implemens a structure MDM and includes the following functions :","category":"page"},{"location":"mdm/#","page":"mdm.jl","title":"mdm.jl","text":"Function Description\nmean_mdm calculates the mean of all the classes in the training set and also    \t\t \t\tnotifies the user if the mean is not convergent in case of some metric \t\t\t\tspaces\nfind_dist finds the distance of each sample case from the so found means of all the classes\npredict_mdm predicts the class for each sample case depending on its distance from the respective means\npredict_prob predicts the probability of each class for each sample case depending on its distance from the respective means\nindCV returns the vectors containing shuffled indices of training and testing samples for each cross validation iteration\ncross_val_mdm implements cross validation for MDM classifier","category":"page"},{"location":"mdm/#","page":"mdm.jl","title":"mdm.jl","text":"For a detailed understanding of mdm, one should know the basics of Riemannian Geometry and its application in the classification of positive definite matrices. One may refer to the following papers for getting the feel of the process.","category":"page"},{"location":"mdm/#","page":"mdm.jl","title":"mdm.jl","text":"A. Barachant, S. Bonnet, M. Congedo, C. Jutten (2012)🎓","category":"page"},{"location":"mdm/#","page":"mdm.jl","title":"mdm.jl","text":"A. Barachant, S. Bonnet, M. Congedo, C. Jutten (2013)🎓","category":"page"},{"location":"mdm/#","page":"mdm.jl","title":"mdm.jl","text":"M. Congedo, A. Barachant, R. Bhatia R (2017a)🎓","category":"page"},{"location":"mdm/#","page":"mdm.jl","title":"mdm.jl","text":"M. Congedo, A. Barachant, E. Kharati Koopaei (2017b)🎓","category":"page"},{"location":"mdm/#","page":"mdm.jl","title":"mdm.jl","text":"Or one may directly look into the Intro to Riemannian Geometry section of the PostDefManifold documentation and quench all their doubts. ","category":"page"},{"location":"mdm/#MDM-structure-1","page":"mdm.jl","title":"MDM structure","text":"","category":"section"},{"location":"mdm/#","page":"mdm.jl","title":"mdm.jl","text":"MDM","category":"page"},{"location":"mdm/#RiemannianML.MDM","page":"mdm.jl","title":"RiemannianML.MDM","text":"This is a structure of the MDM model. It has two attributes namely metric and class_means.\n\nmetric :: Metric        :- The user needs to specify the metric space in which all the distance computations is to be done. The possible options are:\n\nMetric Mean estimation\nEuclidean distance: δ_e; mean: Arithmetic\ninvEuclidean distance: δ_i; mean: Harmonic\nChoEuclidean distance: δ_c; mean: Cholesky Euclidean\nlogEuclidean distance: δ_l; mean: Log Euclidean\nlogCholesky distance: δ_c; mean: Log-Cholesky\nFisher distance: δ_f; mean: Fisher (Cartan, Karcher, Pusz-Woronowicz,...)\nlogdet0 distance: δ_s; mean: LogDet (S, α, Bhattacharyya, Jensen,...)\nJeffrey distance: δ_j; mean: Jeffrey (symmetrized Kullback-Leibler)\nVonNeumann distance: δ_v; mean: Not Availale\nWasserstein distance: δ_w; mean: Wasserstein (Bures, Hellinger, ...)\n\nclass_means             :- This is not to be specified by the user. This comes to play when                               a model is fit with a set of training data so we have to store the means corresponding                               to each of the classes. We can directly access this for an already fit model                               while using functions like predict.\n\n\n\n\n\n","category":"type"},{"location":"mdm/#mean_mdm-1","page":"mdm.jl","title":"mean_mdm","text":"","category":"section"},{"location":"mdm/#","page":"mdm.jl","title":"mdm.jl","text":"mean_mdm","category":"page"},{"location":"mdm/#RiemannianML.mean_mdm","page":"mdm.jl","title":"RiemannianML.mean_mdm","text":"Given a metric, ℍVector( Vector of Hermitian matrices), weights(optional) returns the mean of these Hermitian matrices for mdm classifier.This function is kind of an interface to the mean function of PostDefManifold. Refer to the Mean of PosDefManifold.\n\nArguments :\n\nmetric :: Metric        :-The user needs to specify the metric space in which                               all the distance computations is to be done.\n𝐏::ℍVector              :-Vector of Hermitian matrices or simply a HermitianVector.                                    The vector of points in the Symmetric Positive Definite                                    manifold to be transformed into the the tangent space.\nw::Vector(optional):- Vector containing weights corresponding to every point in 𝐏.\n✓w = true(optional)    :- Boolean to determine whether to calculate weighted mean or                                just take w = []. It also checks if the weights                                sum up to 1. If not does normalization.\n⏩ = false (optional)   :- Boolean to allow threading or not.\n\nReturns :\n\nG :: ℍ                  :- Mean of the set of ℍVector.\n\n\n\n\n\n","category":"function"},{"location":"mdm/#find_dist-1","page":"mdm.jl","title":"find_dist","text":"","category":"section"},{"location":"mdm/#","page":"mdm.jl","title":"mdm.jl","text":"find_dist","category":"page"},{"location":"mdm/#RiemannianML.find_dist","page":"mdm.jl","title":"RiemannianML.find_dist","text":"Given a sample set, classmeans, type of metric returns the distance of each sample case from the points in classmeans. Distance should be caluclated in the metric space opted by the user while creating the instance of mdm.\n\nArguments :\n\nsample :: ℍVector       :-The vector of Hermitian matrices or points in                                      the positive definite manifold for which the                                      prediction is to be made using the model(argument 1)                                      already been trained specially for it.\nclass_means             :- Vector of Hermitian matrices that represent the class                               means or the centroid of all the classes in the training set.\nmetric :: Metric        :-The user needs to specify the metric space in which                               all the distance computations is to be done.\n\nReturn :\n\nA :: Array{Float64,2}   :- Array of distances of each of the sample case from                                   each of the class_means.\n\n\n\n\n\n","category":"function"},{"location":"mdm/#predict_mdm-1","page":"mdm.jl","title":"predict_mdm","text":"","category":"section"},{"location":"mdm/#","page":"mdm.jl","title":"mdm.jl","text":"predict_mdm","category":"page"},{"location":"mdm/#RiemannianML.predict_mdm","page":"mdm.jl","title":"RiemannianML.predict_mdm","text":"Given a sample set, class_means, type of metric returns the predicted classes for each sample case according to the Minimum Distance to the Means scheme.\n\nArguments :\n\nsample :: ℍVector       :- The vector of Hermitian matrices or points in                                          the positive definite manifold for which the                                          prediction is to be made using the model(argument 1)                                          already been trained specially for it.\nclass_means             :- Vector of Hermitian matrices that represent the class                                   means or the centroid of all the classes in the training set.\nmetric :: Metric        :- The user needs to specify the metric space in which                               all the distance computations is to be done.\n\nReturns :\n\nresult :: Array{Int, 1} :- The List of the predicted classes for the given                                   sample set.\n\n\n\n\n\n","category":"function"},{"location":"mdm/#predict_prob-1","page":"mdm.jl","title":"predict_prob","text":"","category":"section"},{"location":"mdm/#","page":"mdm.jl","title":"mdm.jl","text":"predict_prob","category":"page"},{"location":"mdm/#RiemannianML.predict_prob","page":"mdm.jl","title":"RiemannianML.predict_prob","text":"Given a sample set, class_means, type of metric returns the probability of each class for each sample case according to the Minimum Distance to the Means scheme. All the probability sums to 1. This function makes use of the softmax function from thr PostDefManifold to calculate the probability values.\n\nArguments :\n\nsample :: ℍVector       :-The vector of Hermitian matrices or points in                                          the positive definite manifold for which the probability                                          prediction is to be made using the model(argument 1)                                          already been trained specially for it.\nclass_means             :- Vector of Hermitian matrices that represent the class                                   means or the centroid of all the classes in the training set.\nmetric :: Metric        :-The user needs to specify the metric space in which                               all the distance computations is to be done.\n\nReturns :\n\nProb :: Array{Float64,1}:- The List of the predicted probabilities corresponding                                   to each of the classes for the given sample set.\n\n\n\n\n\n","category":"function"},{"location":"mdm/#indCV-1","page":"mdm.jl","title":"indCV","text":"","category":"section"},{"location":"mdm/#","page":"mdm.jl","title":"mdm.jl","text":"indCV","category":"page"},{"location":"mdm/#RiemannianML.indCV","page":"mdm.jl","title":"RiemannianML.indCV","text":"Given the length, nCV( number of cross-validations) returns the vectors containing indices of training and testing samples for each cross validation iteration. This is a helper function to implement crossvalmdm. It uses shuffle! of the Random.jl package. This function holds the prime basis of CrossValidation implementation.\n\nArguments :\n\nk::Int                 :- Last number of the sequence of natural numbers to be                                   shuffled starting from 1\nnCV:: Int              :- Number of cross-validation for which the indices are                                   to be generated.\nshuffle                :- Boolean to inform whether to do shuffling or not.                                Default is set to True.\n\nReturns :\n\nnTest                  :- The size of each testing set.\nnTrain                 :- The size of each training set.\nindTrain               :- The list of all the vectors that contain the training                                   indices for each iteration.\nindTest                :- The list of all the vectors that contain the testing                                   indices for each iteration.\n\n\n\n\n\n","category":"function"},{"location":"mdm/#cross*val*mdm-1","page":"mdm.jl","title":"crossvalmdm","text":"","category":"section"},{"location":"mdm/#","page":"mdm.jl","title":"mdm.jl","text":"cross_val_mdm","category":"page"},{"location":"mdm/#RiemannianML.cross_val_mdm","page":"mdm.jl","title":"RiemannianML.cross_val_mdm","text":"Given a ℍVector(training set), y(labels), cv(number of cross-validations) returns the the list containing the score for each cross-validation iteration. The score may be average balanced accuracy or average regular accuracy depending on the choice of the user. *It is better to call for balanced accuracy as a better estimation of the model performance. Overall, if the number of ases in for each class are same, balanced accuracy equals the regular accuracy.\n\nThis is the main function implementing cross validation for MDM classifier. It uses indCV as its basic helper function. It also returns the final confusion matrix.\n\nArguments :\n\n𝐗 :: ℍVector          :- The training set of type Vector of Hermitian matrices.\ny                      :- The list of labels corresponding to each trial\ncv :: Int              :- The number of cross-validation desired by the user\nscoring :: String      :- If Balanced Accuracy is requied or the Regular Accuracy.                                   The default is set to Balanced Accuracy.\nmetric :: Metric       :- The metric space in which to do the computations. To be                                   specified by the user as one of the many metric spaces options provided in PostDefManifold.                                   One may refer to [mdm.jl documentations] for exploring all the possible options.                                   The default is set to Fisher.\ncnfmat :: Bool         :- Boolean to notify if the user wants the confusion matrix also.                              Default is set to false.\n\nReturns :\n\ncross_val score       :- The list of the balanced accuracy score for each                               cross_val iteration.\n\n\n\n\n\n","category":"function"},{"location":"example/#example.jl-1","page":"example.jl","title":"example.jl","text":"","category":"section"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"This unit demonstrates the use of RiemannianML using various examples. It can be used as a referrence guide while working in RiemannianML. Examples are provided for all the functions and structure objects. The corresponding unit example.jl contains the code corresponding to each and every example citation, one may directly copy these codes from there and run on their own to check its working. The results cited here might be different then the ones obtained by the user because random points generation is done to create stimulated data, so it may vary every time but the overall range varies roughly over the same stretch. ","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"For these examples below, simulated data are used. These data are created by few simple lines of codes. Let us assume that the number of eclectrodes in our situation is 30.","category":"page"},{"location":"example/#Simulated-data-creation-(not-necessary)-1","page":"example.jl","title":"Simulated data creation (not necessary)","text":"","category":"section"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"So, we fix n = 30. This implies our data will consist of 30 x 30 Hermitian matrices. For the real cases these simply be Real matrices.","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"n=30","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"In the following examples, binary classification is demonstrated. Let the training examples for each of the two classes be 80. So, k1 = 80 and k2 = 80. The total training set size =k1 + k2","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"k1=80\r\nk2=80\r\nk=k1+k2","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"To create stimulated data for EEG, we randomly pick two points A1, A2 in the Positive Definite manifold. Let A1 and A2 be the standard cases representing class 1 and class 2 respectively.","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"A1=randP(n)\r\nA2=randP(n)","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"For creating the entire training set, that could behave as the training samples for classes 1 and 2. These should be similar somewhat to either of A1 or A2. To ensure this, first of all a set containing k1 + k2 random points in the post def manifold is generated.","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"P=randP(n, k1+k2)","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"Now, we move each of these points closer to either A1 or A2 so they have some resemblence to them. This will ensure that our data is worth classifying. We move these points slowly and slowly closer to the standards chosen( A1, A2 ). This closeness is monitored by the value gm here. gm = 0 means, no shifting is done, they are just random set of points. gm = 1 means, all the points are shifted exactly to A1 or A2. All the intermediate values of gm will take the points to intermediate distance ratios on their geodesic joining A1/A2. So, half of the points are taken closer to A1, and the other half to A2. This means half are closer to class 1 while the other half to class 2.","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"gm=0.1\r\nP2=[geodesic(Fisher, P[i], A1, gm) for i=1:k1]\r\nQ2=[geodesic(Fisher, P[i], A2, gm) for i=k1+1:k1+k2 ]\r\n𝐗=ℍVector([P2; Q2])","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"The new shifted set of points are contained in P2 and Q2. We concatenate them into one set 𝐗, which represent now the simulated training set. Then the label is created for this training set.","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"Y = [repeat([1], k1); repeat([2],k2)]","category":"page"},{"location":"example/#Model-declaration-1","page":"example.jl","title":"Model declaration","text":"","category":"section"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"Like in ScikitLearn, for applying a model on your data, first of all create an instance or object of the corresponding model class with all specifications. Here they are not classes but structures.","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"Model instance for kneighborClf that does KNeighborsClassifier classification.","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"model1 = kneighborClf(n_neighbors=3)","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"Model instance for LogisticReg that applies LogisticRegression.","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"model2 = LogisticReg()","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"Model instance for LinearSVM that applies LinearSVC.","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"model3 = LinearSVM()","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"Model instance for SVM that applies SVC.","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"model4 = SVM()","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"Model instance for MDM( Minimum Distance to Mean ) that applies mdm classification.","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"model5 = MDM(Fisher)","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"Model instance for LogisticRegCV that applies LogisticRegressionCV( CV - cross-validation ).","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"model6 = LogisticRegCV()","category":"page"},{"location":"example/#Fitting-to-the-model-1","page":"example.jl","title":"Fitting to the model","text":"","category":"section"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"The so created simulated training sets are then fit to the models. This is done simply by calling the fit! function that takes 3 arguments, model, training set and labels. fit! is the first function i.e. to be called so that our model is ready to make predictions.","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"fit!(model1, 𝐗,Y)\r\nfit!(model2, 𝐗,Y)\r\nfit!(model5, 𝐗,Y)\r\nfit!(model3, 𝐗,Y)\r\nfit!(model4, 𝐗,Y)\r\nfit!(model6, 𝐗,Y)","category":"page"},{"location":"example/#Cross-Validation-1","page":"example.jl","title":"Cross-Validation","text":"","category":"section"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"In order to evaluate the model performance, cross-validation is done. The score of cross-validation is returned by the crossvalscore function which is then printed here.","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"println(cross_val_score(model1,𝐗,Y, cv = cv_fold))\r\nprintln(cross_val_score(model2,𝐗,Y, cv = cv_fold))\r\nprintln(cross_val_score(model5,𝐗,Y, cv = cv_fold))","category":"page"},{"location":"example/#Making-prediction-using-the-predict-function-1","page":"example.jl","title":"Making prediction using the predict function","text":"","category":"section"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"Now, in order to make the predictions, predict function is to be employed. Here also, a simulated sample set is feeded into the predict function to check if the prediction is made correctly or not. Again the same procedure as the one used for training set generation, is followed. After doing the desired amount of shifting, the points are stored in samp.","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"T = randP(n,5)\r\nS = randP(n,3)\r\ntm = 0.198\r\nT1 = [geodesic(Fisher, T[i], A1, tm) for i=1:length(T)]\r\nS1 = [geodesic(Fisher, S[i], A2, tm) for i=1:length(S)]\r\nsamp = ℍVector([T1; S1])","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"Calling the predict function on the testing sample samp with different fit models.","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"predict(model2, samp)\r\npredict(model5, samp)","category":"page"},{"location":"example/#Table-representing-performance-score-for-all-the-models-with-simulated-data.-1","page":"example.jl","title":"Table representing performance score for all the models with simulated data.","text":"","category":"section"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"A table is constructed that holds the cross-validation score for each of the models corresponding to different training sets. These different training sets are simulated training sets for an increasing value of gm i.e. extent of shifting. For this all the models are put in the list model. The following for loop fills values into this table. The table is printed.","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"model = [model1, model2, model3, model4, model5, model6]\r\nprintln(model)\r\ncv_fold = 4\r\ntable = Array{Float64, 2}(undef, 6,9)\r\nfor i = 1:6\r\n    for j = 1:9\r\n        gm = 0.035 * j\r\n        P2=[geodesic(Fisher, P[i], A1, gm) for i=1:k1]\r\n        Q2=[geodesic(Fisher, P[i], A2, gm) for i=k1+1:k1+k2 ]\r\n        𝐗=ℍVector([P2; Q2])\r\n        #fit!(model[i], 𝐗,Y )\r\n        table[i,j] = (𝚺(cross_val_score(model[i],𝐗,Y, cv = cv_fold)))/ cv_fold\r\n    end\r\nend\r\n\r\nprintln(table)\r\n\n\n\n>> [0.493827 0.512346 0.556268 0.625594 0.719611 0.825024 0.90622 0.974834 0.993827; \r\n0.650285 0.937559 0.987654 0.993827 0.993827 1.0 1.0 1.0 1.0; \r\n1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; \r\n0.524454 0.54321 0.655508 0.755223 0.855413 0.981007 0.993827 0.993827 1.0; \r\n0.59375 0.55 0.6 0.7875 0.8625 0.975 0.9875 0.99375 1.0; \r\n1.0 1.0 1.0 1.0 1.0 0.993827 1.0 1.0 1.0]","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"Where as for a value of gm = 0.015 * [1:9], we get the following set of values. Values are tabulated.","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"Model\\gm 0.015 0.030 0.045 0.060 0.075 0.900 0.105 0.120 0.135\nkneighbor 0.49375 0.4875 0.49375 0.5125 0.5125 0.53125 0.5375 0.575 0.625\nLogisticReg(lasso) 0.54375 0.7 0.83125 0.93125 0.98125 0.99375 0.99375 0.99375 0.99375\nLinearSVM 0.89375 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\nSVM 0.53125 0.53125 0.55625 0.56875 0.59375 0.61875 0.6625 0.68125 0.71875\nMDM 0.49375 0.53125 0.5375 0.5875 0.6625 0.65625 0.63125 0.65625 0.6875\nLogisticRegCV(lasso) 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0","category":"page"},{"location":"example/#Table-representing-performance-score-for-all-the-models-with-real-data.-1","page":"example.jl","title":"Table representing performance score for all the models with real data.","text":"","category":"section"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"For real data taken from BNCI2014001 :","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"The dataset contains the recording of 9 sujbects during two different sessions. Each is stored in a different file and has a different subject number.","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"1 = subject 1, session 1\n2 = subject 1, session 2\n\n\n\n17 = subject 9, session 1\n18 = subject 9, session 2","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"We get the following results. Tabulation is done for few subjects only.","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"Model\\gm kneighbor LogisticReg(lasso) LinearSVM SVM MDM LogisticRegCV(lasso)\nsubject_1 0.770833 0.930556 0.861111 0.798611 0.861111 0.909722\nsubject_2 0.819444 0.902778 0.895833 0.861111 0.888889 0.909722\nsubject_3 0.451389 0.645833 0.645833 0.590278 0.611111 0.618056\nsubject_4 0.583333 0.618056 0.666667 0.513889 0.541667 0.638889\nsubject_15 0.930556 0.972222 0.965278 0.930556 0.951389 0.965278\nsubject_16 0.930556 0.972222 0.965278 0.958333 0.965278 0.958333\nsubject_17 0.784722 0.840278 0.8125 0.722222 0.791667 0.826389\nsubject_18 0.868056 0.902778 0.875 0.881944 0.847222 0.909722","category":"page"},{"location":"tutorial/#Tutorial-1","page":"Tutorial","title":"Tutorial","text":"","category":"section"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"This is a tutorial guide for RiemannianML module. If you hold previous experience in working with ScikitLearn in Python, you may skip this tutorial. For rest, you may have a look. ","category":"page"},{"location":"tutorial/#Classification-in-RiemannianML-1","page":"Tutorial","title":"Classification in RiemannianML","text":"","category":"section"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"We begin with deciding the model for our data. Currently this package serves 5 models/classifiers:","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"K Nearest Neighbor\nLogistic Regression \nLogistic Regression with Cross-Validation\nLinear SVC( Support Vector Classification)\nSVC( Support Vector Classification)\nMDM( Minimum Distance to Mean) in Positive Definite Manifold","category":"page"},{"location":"tutorial/#Creating-Models-1","page":"Tutorial","title":"Creating Models","text":"","category":"section"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"The user has to create an object/instance for the classifier class(here structure). The user can create an instance by specifying all the available arguments. Since, this module sets a uniform interface between ScikitLearn python,all the arguments are accepted in ScikitLearn python for all the classifiers except MDM are are accepted here also.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"# Model instance for kneighborClf that does KNeighborsClassifier classification.\r\nmodel1 = kneighborClf(n_neighbors=3)\r\n\n# Model instance for LogisticReg that applies LogisticRegression.\r\nmodel2 = LogisticReg(solver = \"liblinear\", max_iter = 4000)\r\n\n# Model instance for LinearSVM that applies LinearSVC.\r\nmodel3 = LinearSVM()\r\n\n# Model instance for SVM that applies SVC.\r\nmodel4 = SVM()\r\n\n# Model instance for MDM( Minimum Distance to Mean ) that applies mdm classification.\r\nmodel5 = MDM(Fisher)\r\n\n# Model instance for LogisticRegCV that applies LogisticRegressionCV\r\n# (CV - cross-validation).\r\nmodel6 = LogisticRegCV(solver = \"liblinear\", max_iter = 4000, cv = 5)","category":"page"},{"location":"tutorial/#Loading-the-data-1","page":"Tutorial","title":"Loading the data","text":"","category":"section"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"After a classifier instance is created we need to fit the classifier model with the data. For this we first need to load the data in our code. Say my data is stored in npz format(.npz) in my local computer at the following address.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"using NPZ\r\n\npath = \"/home/saloni/RiemannianML/src/\" # where files are stored\r\nfilename = \"subject_1.npz\" # for subject number i\r\ndata = npzread(path*filename)\r\nX = data[\"data\"] # retrive the epochs\r\ny = data[\"labels\"] # retrive the corresponding labels","category":"page"},{"location":"tutorial/#Making-the-data-ready-for-use-1","page":"Tutorial","title":"Making the data ready for use","text":"","category":"section"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"This corresponds to the training part. The training data should be in the form of :","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"𝐗 :-  Vector of Hermitian Matrices. These Hermitian Matrices are the covariance matrices formed   \t\t    out of the raw data. The raw data of signals first need to be converted into their \t\t   corresponding covariance matrices. This can be very easily done using the gram function of \t\t    PosDefManifold. This is what is done in the below code.     \ny :-  Labels corresponding to each training sample. Labels should be integers from 1 to n, where  \t\t   n is the number of classes.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"train_size = size(X,1)\r\n𝐗 = ℍVector(undef, sam_size)\r\n@threads for i = 1:train_size\r\n\t \t𝐗[i] = gram(X[i,:,:])\r\nend","category":"page"},{"location":"tutorial/#Fitting-your-data-to-the-model-1","page":"Tutorial","title":"Fitting your data to the model","text":"","category":"section"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Once you are ready with your data, you are all set to train your model. This is done easily using the fit! function.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"fit!(model1,𝐗 ,y)\r\nfit!(model2,𝐗 ,y)","category":"page"},{"location":"tutorial/#Making-predictions-1","page":"Tutorial","title":"Making predictions","text":"","category":"section"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"We say now that the model has been trained, it can be used to make predictions for the test cases. This is again done using a simple function, predict. Say we have the test cases stored the same way as the training data. We load it the same way and apply the same gram function to convert them into covariance matrices. Now we again have a vector of Hermitian Matrices as our testing set 𝐓. We predict the class for each sample in the testing set. We can only give a trained model as input.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"predict(model1,𝐓)\r\npredict(model2,𝐓)","category":"page"},{"location":"tutorial/#Evaluating-the-models-1","page":"Tutorial","title":"Evaluating the models","text":"","category":"section"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"Last but not the least, we can check the performance of our model by cross-validation. For models imported from ScikitLearn.jl we can simply calculate the score also using the score! function.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"score!(model1.clf, 𝐗 ,y)","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"But its always better to evaluate a model's performance by cross-validation and not simply calculating the score. So, we make use of cross_val_score. We directly feed the model into it without even training it before. This function returns The list containing the score for each cross-validation iteration. The number of iteration can be managed using the cv argument.","category":"page"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"println(cross_val_score(model1,𝐗,Y, cv = 5))\r\n\n# Prints the average of all the cross-validation scores\r\nprintln(𝚺(cross_val_score(model2,𝐗,Y, cv = 5))/ cv)\r\n\n# Special arguments available only for mdm model types\r\nprintln(cross_val_score(model5,𝐗,Y, cv = 5, cnfmat = true))","category":"page"},{"location":"tutorial/#Example(-continuing-from-above)-1","page":"Tutorial","title":"Example( continuing from above)","text":"","category":"section"},{"location":"tutorial/#","page":"Tutorial","title":"Tutorial","text":"model = [model1, model2, model3, model4, model5, model6]\r\ncv_fold = 4\r\ntable = Array{Float64, 2}(undef, 6, cv_fold)\r\nfor i = 1:6\r\n\t\ttable[i,:] = (cross_val_score(model[i],𝐗,y, cv = cv_fold))\r\nend\r\nprintln([𝚺(table[i,:])/cv_fold for i = 1:6])\r\n\n\n>>> [0.493056, 0.673611, 0.701389, 0.5, 0.708333, 0.715278]","category":"page"}]
}
