var documenterSearchIndex = {"docs":
[{"location":"#RiemannianML-Documentation-1","page":"RiemannianML Documentation","title":"RiemannianML Documentation","text":"","category":"section"},{"location":"#Requirements-1","page":"RiemannianML Documentation","title":"Requirements","text":"","category":"section"},{"location":"#","page":"RiemannianML Documentation","title":"RiemannianML Documentation","text":"Julia version ‚â• 1.0.3 Python ScikitLearn (Refer for installation) ","category":"page"},{"location":"#Installation-1","page":"RiemannianML Documentation","title":"Installation","text":"","category":"section"},{"location":"#","page":"RiemannianML Documentation","title":"RiemannianML Documentation","text":"Execute the following command in Julia's REPL:","category":"page"},{"location":"#","page":"RiemannianML Documentation","title":"RiemannianML Documentation","text":"]add RiemannianML","category":"page"},{"location":"#","page":"RiemannianML Documentation","title":"RiemannianML Documentation","text":"To obtain the latest development version execute instead","category":"page"},{"location":"#","page":"RiemannianML Documentation","title":"RiemannianML Documentation","text":"]add RiemannianML#master","category":"page"},{"location":"#Overview-1","page":"RiemannianML Documentation","title":"Overview","text":"","category":"section"},{"location":"#","page":"RiemannianML Documentation","title":"RiemannianML Documentation","text":"Riemannian geometry studies smooth manifolds, multi-dimensional curved spaces with peculiar geometries endowed with non-Euclidean metrics. In these spaces Riemannian geometry allows the definition of angles, geodesics (shortest path between two points), distances between points, centers of mass of several points, etc.","category":"page"},{"location":"#","page":"RiemannianML Documentation","title":"RiemannianML Documentation","text":"In several fields of research such as computer vision and brain-computer interface, treating data in the P manifold has allowed the introduction of machine learning approaches with remarkable characteristics, such as simplicity of use, excellent classification accuracy, as demonstrated by the winning score obtained in six international data classification competitions, and the ability to operate transfer learning (Congedo et al., 2017a, Congedo et al., 2017b).","category":"page"},{"location":"#","page":"RiemannianML Documentation","title":"RiemannianML Documentation","text":"In this package we are concerned with making use of Riemannian Geometry in classification of data in the manifold of Positive Definite Matrices. This can be done in two ways, either in the Positive Definite Manifold or in the Eucledian Space of transformed data.   ","category":"page"},{"location":"#","page":"RiemannianML Documentation","title":"RiemannianML Documentation","text":"Positive Definite Manifold : Here we can use different distance metrics to compute the distances between the points represented by positive definite matrices. Using thsi we apply MDM(Minimum Distance to Mean) criteria for classifying test data in the positive defininite manifold.\nEucledian Space : Here the data points from the manifold of positive definite matrices are transformed into corresponding points in the tangent space of the mean point of the set. Since the manifold of positive definite matrices forms a Riemannian manifold, so this brings us the opportunity to implement all the machine learning algorithms from scikit-learn. The reason being that the tangent space behaves like an Eucledian space, so it opens the way to all the general machine learning algorithms implemented in sckit-learn as they assume the metric apace to be Eucledian. The transformation is done using a specific relation()","category":"page"},{"location":"#","page":"RiemannianML Documentation","title":"RiemannianML Documentation","text":"This package makes extensive use of the functions from the package PostDefManifold. The reader might have a look into this for a better understanding of the functions used. ","category":"page"},{"location":"#","page":"RiemannianML Documentation","title":"RiemannianML Documentation","text":"For a formal introduction to the P manifold the reader is referred to the monography written by Bhatia (2007).","category":"page"},{"location":"#","page":"RiemannianML Documentation","title":"RiemannianML Documentation","text":"For an introduction to Riemannian geometry and an overview of mathematical tools implemented in this package, see Intro to Riemannian Geometry in the documentation of PostDefManifold.","category":"page"},{"location":"#","page":"RiemannianML Documentation","title":"RiemannianML Documentation","text":"For starting using this package, browse the code units listed here below and execute the many code examples you will find therein.","category":"page"},{"location":"#Code-units-1","page":"RiemannianML Documentation","title":"Code units","text":"","category":"section"},{"location":"#","page":"RiemannianML Documentation","title":"RiemannianML Documentation","text":"RiemannianML includes six code units (.jl files):","category":"page"},{"location":"#","page":"RiemannianML Documentation","title":"RiemannianML Documentation","text":"Unit Description\nMainModule (RiemannianML.jl) Main module, constants, types, aliases, tips & tricks\nknn.jl Unit implementing Kneighbhor Classification\nlogisticRegression.jl Unit implementing LogisticRegression and LogisticRegressionCV\nSVM.jl Unit implementing LinearSVC and SVC\nmdm.jl Unit implementing MDM( Minimum Distance to Mean) classification\ncross_mdm.jl Unit implementing cross-validation for MDM classifier\ntrain_test.jl Unit implementing the tranformation function. Along with overwriting fit!, predict and crossvalscore from the ScikitLearn.jl package\nexample.jl Unit containing examples for understanding and execution","category":"page"},{"location":"#Contents-1","page":"RiemannianML Documentation","title":"Contents","text":"","category":"section"},{"location":"#","page":"RiemannianML Documentation","title":"RiemannianML Documentation","text":"Pages = [       \"index.md\",\r\n                \"MainModule.md\",\r\n                \"knn.md\",\r\n                \"logisticRegression.md\",\r\n                \"SVM.md\",\r\n                \"mdm.md\",\r\n                \"cross_mdm.md\",\r\n\t\t\"train_test.md\",\r\n\t\t\"example.md\"]\r\nDepth = 1","category":"page"},{"location":"#Index-1","page":"RiemannianML Documentation","title":"Index","text":"","category":"section"},{"location":"#","page":"RiemannianML Documentation","title":"RiemannianML Documentation","text":"","category":"page"},{"location":"MainModule/#MainModule-(RiemannianML.jl)-1","page":"MainModule (RiemannianML.jl)","title":"MainModule (RiemannianML.jl)","text":"","category":"section"},{"location":"MainModule/#","page":"MainModule (RiemannianML.jl)","title":"MainModule (RiemannianML.jl)","text":"This is the main unit containing the RiemannianML module.","category":"page"},{"location":"MainModule/#","page":"MainModule (RiemannianML.jl)","title":"MainModule (RiemannianML.jl)","text":"It uses the following standard Julia packages:","category":"page"},{"location":"MainModule/#","page":"MainModule (RiemannianML.jl)","title":"MainModule (RiemannianML.jl)","text":"using\nLinearAlgebra\nStatistics\nPostDefManifold\nScikitLearn\nScikitLearn.CrossValidation\nRandom","category":"page"},{"location":"MainModule/#","page":"MainModule (RiemannianML.jl)","title":"MainModule (RiemannianML.jl)","text":"The main module does not contains functions, but it declares all imports and exports of Julia functions used or created in all the units.","category":"page"},{"location":"knn/#knn.jl-1","page":"knn.jl","title":"knn.jl","text":"","category":"section"},{"location":"knn/#","page":"knn.jl","title":"knn.jl","text":"This module implements the KNeighborsClassifier model for the data points in the Riemannian manifold of Symmetric Positive Definite (SPD). The structure below is similar to a class of KNeighborsClassifier we have in ScikitLearn in Python. The user has to create an object/instance for the classifier class(here structure). The user can create an instance of all the desired specifications.The specifications are added as atributes to this structure. For further information on kneighborClf, one may refer to the scikit-learn documentation of KNeighborsClassifier.","category":"page"},{"location":"knn/#","page":"knn.jl","title":"knn.jl","text":"An object of scikit-learn KNeighborsClassifier is automatically created by the constructor as soon as an object of this struct is created by the user. This structure is created to that the same fit! function from ScikitLearn.jl could be used. To overwrite fit! with the same number of sttributes and similar type we needed to make a change in the argument types, but the sample labels could not differ and the training samples are not of any specified type in ScikitLearn.jl. This gives rise to ambiguities. So, the only option was to change the model type. Now our fit!(the one we have written) takes a julia structure, training samples and labels y. This difference solves the ambiguity between the two available fit! options.","category":"page"},{"location":"knn/#kneighborClf-structure-1","page":"knn.jl","title":"kneighborClf  structure","text":"","category":"section"},{"location":"knn/#","page":"knn.jl","title":"knn.jl","text":"kneighborClf","category":"page"},{"location":"knn/#RiemannianML.kneighborClf","page":"knn.jl","title":"RiemannianML.kneighborClf","text":"Structure for implementing KNeighborsClassifier, with all the arguments as attributes to this structure. The 'clf' attribute is the python object of KNeighborsClassifier that is constructed by the constructor function of this structure as soon as a kneighborClf instance is created by the user.\n\nThe available specifications for creating a model of kneighborClf are :\n\nn_neighbors :: Int64\nweights:: String\nalgorithm:: String\nleaf_size:: Int64\np:: Int64\nmetric:: String\nmetric_params :: Dict\nn_jobs :: Int\n\nOne should refer to the ScikitLearn documentation of KNeighborsClassifier whose link is provided above for the better understanding of these parameters with respect to KNeighbors Classification.\n\n## Example\nusing RiemannianML\nmodel = kneighborClf(n_neighbors=3)\n\n\n\n\n\n","category":"type"},{"location":"logisticRegression/#logisticRegression.jl-1","page":"logisticRegression.jl","title":"logisticRegression.jl","text":"","category":"section"},{"location":"logisticRegression/#","page":"logisticRegression.jl","title":"logisticRegression.jl","text":"This unit implements the LogisticRegression models for the data points in the Riemannian manifold of Symmetric Positive Definite (SPD). The structures below are similar to classes of LogisticRegression and LogisticRegressionCV we have in ScikitLearn in Python. The user has to create an object/instance for the classifier class(here structure). The user can create a model instance of all the desired specifications.The specifications are added as atributes to these structures. This Module incorporates two models of LogisticRegression :     - LogisticReg -   Simple logisticRegression in which the value of the penalty                        coefficient alpha is set manually by the user. The user is                        responsible for choosing a specific value of alpha.     - LogisticRegCV - LogisticRegression with cross validation, to find the best                        suit for alpha. The user can provide a range of alpha and the                        algorithm finds the best suited value of alpha out of those.","category":"page"},{"location":"logisticRegression/#","page":"logisticRegression.jl","title":"logisticRegression.jl","text":"For further information on LogisticReg and LogisticRegCV one may refer to the scikit-learn    documentations of LogisticRegression     and LogisticRegressionCV     respectively.","category":"page"},{"location":"logisticRegression/#","page":"logisticRegression.jl","title":"logisticRegression.jl","text":"An object of scikit-learn LogisticRegression or LogisticRegressionCV is automatically created by the constructor as soon as an instance of their structs is created by the user. These structures are implemented so that the same fit! function from ScikitLearn.jl could be used. To overwrite fit! with the same number of sttributes and similar type we needed to make a change in the argument types, but the sample labels could not differ and the training samples are not of any specified type in ScikitLearn.jl. This gives rise to ambiguities. So, the only option was to change the model type. Now our fit!(the one we have written) takes a julia structure, training samples and labels y. This difference solves the ambiguity between the two available fit! options.","category":"page"},{"location":"logisticRegression/#LogisticReg-structure-1","page":"logisticRegression.jl","title":"LogisticReg structure","text":"","category":"section"},{"location":"logisticRegression/#","page":"logisticRegression.jl","title":"logisticRegression.jl","text":"LogisticReg","category":"page"},{"location":"logisticRegression/#RiemannianML.LogisticReg","page":"logisticRegression.jl","title":"RiemannianML.LogisticReg","text":"Structure for implementing LogisticRegression, with all the arguments as attributes to this structure. The 'clf' attribute is the python object of LogisticRegression that is constructed by the constructor function of this structure as soon as a LogisticReg instance is created by the user.\n\nThe available specifications for creating a model of LogisticReg are :\n\npenalty:: String\ndual :: Bool\ntol :: Float64\nC :: Float64\nfit_intercept :: Bool\nintercept_scaling :: Float64\nclass_weight :: Dict\nrandom_state :: Int\nsolver :: String\nmax_iter :: Int64\nmulti_class :: String\nverbose :: Int64\nwarm_start :: Bool\nn_jobs :: Int\nl1_ratio :: Float64\n\nOne should refer to the ScikitLearn documentation of LogisticRegression whose link is provided above for the better understanding of these parameters with respect to LogisticRegression.\n\n## Example\nusing RiemannianML\nmodel = LogisticReg()\n\n\n\n\n\n","category":"type"},{"location":"logisticRegression/#LogisticRegCV-structure-1","page":"logisticRegression.jl","title":"LogisticRegCV structure","text":"","category":"section"},{"location":"logisticRegression/#","page":"logisticRegression.jl","title":"logisticRegression.jl","text":"LogisticRegCV","category":"page"},{"location":"logisticRegression/#RiemannianML.LogisticRegCV","page":"logisticRegression.jl","title":"RiemannianML.LogisticRegCV","text":"Structure for implementing LogisticRegressionCV, with all the arguments as attributes to this structure. The 'clf' attribute is the python object of LogisticRegressionCV that is constructed by the constructor function of this structure as soon as a LogisticRegCV instance is created by the user.\n\nThe available specifications for creating a model of LogisticRegCV are :\n\nCs :: Int[] or Float64[]\nfit_intercept :: Bool\ncv :: Int\ndual :: Bool\npenalty :: String\nscoring :: String\nsolver :: String\ntol :: Float64\nmax_iter :: Int\nclass_weight :: Dict\nn_jobs :: Int\nverbose :: Int\nrefit :: Bool\nintercept_scaling :: Float64\nmulti_class :: String\nrandom_state :: Int\nl1_ratios :: Float64\n\nOne should refer to the ScikitLearn documentation of LogisticRegressionCV whose link is provided above for the better understanding of these parameters with respect to LogisticRegressionCV.\n\n## Example\nusing RiemannianML\nmodel = LogisticRegCV()\n\n\n\n\n\n","category":"type"},{"location":"SVM/#SVM.jl-1","page":"SVM.jl","title":"SVM.jl","text":"","category":"section"},{"location":"SVM/#","page":"SVM.jl","title":"SVM.jl","text":"This module implements the Support Vector models for the data points in the manifold of positive definite matrices. The structures below are similar to classes of LinearSVC and SVC we have in ScikitLearn in Python. The user has to create an object/instance for the classifier class(here structure). The user can create a model instance of all the desired specifications.The specifications are added as atributes to these structures. This Module incorporates two models of Support Vector Machine type :     - LinearSVM     - SVM","category":"page"},{"location":"SVM/#","page":"SVM.jl","title":"SVM.jl","text":"For further information on LinearSVM and SVM one may refer to the scikit-learn documentations of LinearSVC and SVC respectively.","category":"page"},{"location":"SVM/#","page":"SVM.jl","title":"SVM.jl","text":"An object of scikit-learn LinearSVC or SVC is automatically created by the constructor as soon as an instance of their structs is created by the user. These structures are implemented so that the same fit! function from ScikitLearn.jl could be used. To overwrite fit! with the same number of sttributes and similar type we needed to make a change in the argument types, but the sample labels could not differ and the training samples are not of any specified type in ScikitLearn.jl. This gives rise to ambiguities. So, the only option was to change the model type. Now our fit!(the one we have written) takes a julia structure, training samples and labels y. This difference solves the ambiguity between the two available fit! options.","category":"page"},{"location":"SVM/#LinearSVM-structure-1","page":"SVM.jl","title":"LinearSVM  structure","text":"","category":"section"},{"location":"SVM/#","page":"SVM.jl","title":"SVM.jl","text":"LinearSVM","category":"page"},{"location":"SVM/#RiemannianML.LinearSVM","page":"SVM.jl","title":"RiemannianML.LinearSVM","text":"Structure for implementing LinearSVC, with all the arguments as attributes to this structure. The 'clf' attribute is the python object of LinearSVC that is constructed by the constructor function of this structure as soon as a LinearSVM instance is created by the user.\n\nThe available specifications for creating a model of LinearSVM are :\n\npenalty:: String\nloss :: String\ndual :: Bool\ntol :: Float64\nC :: Float64\nmulti_class :: String\nfit_intercept :: Bool\nintercept_scaling :: Float64\nclass_weight :: Dict\nverbose :: Int64\nrandom_state :: Int\nmax_iter :: Int64\n\nOne should refer to the ScikitLearn documentation of LinearSVC whose link is provided above for the better understanding of these parameters with respect to Linear Support Vector Machine.\n\n## Example\nusing RiemannianML\nmodel = LinearSVM()\n\n\n\n\n\n","category":"type"},{"location":"SVM/#SVM-structure-1","page":"SVM.jl","title":"SVM  structure","text":"","category":"section"},{"location":"SVM/#","page":"SVM.jl","title":"SVM.jl","text":"SVM","category":"page"},{"location":"SVM/#RiemannianML.SVM","page":"SVM.jl","title":"RiemannianML.SVM","text":"Structure for implementing SVC, with all the arguments as attributes to this structure. The 'clf' attribute is the python object of SVC that is constructed by the constructor function of this structure as soon as a SVM instance is created by the user.\n\nThe available specifications for creating a model of SVM are :\n\npenalty:: String\nloss :: String\ndual :: Bool\ntol :: Float64\nC :: Float64\nmulti_class :: String\nfit_intercept :: Bool\nintercept_scaling :: Float64\nclass_weight :: Dict\nverbose :: Int64\nrandom_state :: Int\nmax_iter :: Int64\n\nOne should refer to the ScikitLearn documentation of SVC whose link is provided above for the better understanding of these parameters with respect to Support Vector Machine.\n\n## Example\nusing RiemannianML\nmodel = SVM()\n\n\n\n\n\n","category":"type"},{"location":"train_test/#train_test.jl-1","page":"train_test.jl","title":"train_test.jl","text":"","category":"section"},{"location":"train_test/#","page":"train_test.jl","title":"train_test.jl","text":"This unit implements the tranformation function. Along with that this unit overwrites the fit!, predict and crossvalscore from the ScikitLearn.jl package. This enables us to use the same functions even for data in the manifold of positive definite matrices.","category":"page"},{"location":"train_test/#","page":"train_test.jl","title":"train_test.jl","text":"It imports the required machine learning models from scikit-learn python using PyCall. This unit includes the following functions :","category":"page"},{"location":"train_test/#","page":"train_test.jl","title":"train_test.jl","text":"Function Description\ntransformts internal function that projects the points in the SPD manifold into the tangent space\nfit! fits the model for the given training set\npredict makes prediction for the points in the test set\ncross_val_score evaluates the cross-validation score of the estimator or model","category":"page"},{"location":"train_test/#*transform*ts-1","page":"train_test.jl","title":"transformts","text":"","category":"section"},{"location":"train_test/#","page":"train_test.jl","title":"train_test.jl","text":"_transform_ts","category":"page"},{"location":"train_test/#RiemannianML._transform_ts","page":"train_test.jl","title":"RiemannianML._transform_ts","text":"This is an internal function which performs the transformation of data from the manifold of positive definite matrices to the eucledian space of the data set mean. We find the mean of the entire data set with the help of the mean function from PostDefManifold. Once the mean is speculated we do the transformation of all the data points to their corresponding values in the tangent space of the data set mean. The relation employed for transformation is the following logarithmic relation:         (G¬Ω * log(‚Ñç(G‚Åª¬Ω * ùêè[i] * G‚Åª¬Ω)) * G¬Ω) where G is the data set mean and ùêè[i] the set of points to be transformed. For the better understanding of this transformation, one may refer to the papers.\n\nArguments taken by this function are:\n\nùêè::‚ÑçVector              :- Vector of Hermitian matrices or simply a HermitianVector.                                    The vector of points in the Symmetric Positive Definite                                    manifold to be transformed into the the tangent space.\nw::Vector(optional):- Vector containing weights corresponding to every point                                    in ùêè.\n‚úìw = true(optional)     :- Boolean to determine whether to calculate weighted mean                                    or just take w = [].\n‚è© = false (optional)    :- Boolean to allow threading or not.\n\nReturn Value:\n\nVec :: Array{Float64, 2} :-Vector of all the points in the training set.\n\n\n\n\n\n","category":"function"},{"location":"train_test/#fit!-1","page":"train_test.jl","title":"fit!","text":"","category":"section"},{"location":"train_test/#","page":"train_test.jl","title":"train_test.jl","text":"fit!","category":"page"},{"location":"train_test/#ScikitLearnBase.fit!","page":"train_test.jl","title":"ScikitLearnBase.fit!","text":"This function is an overwriting of the default fit! function available in the ScikitLearn.jl package. It checks the type of model, if it is mdm it runs a block of code that fits the data which is in positive definite matrices manifold directly by finding mean of datasets from all the classes. This mean list is stored in the classmeans attribute of the mdm instance. If the model is not of type mdm, it takes a different and simpler path. Then the function just calls the internal function _transformts to make the transformation into the tangent space. This tangent space behaves like an eucledian space. So, now the default fit! of ScikitLearn.jl can directly be put to use. The function also prints the average regular score in this case.\n\nArguments taken by this function are:\n\nmodel::RiemannianML object:- Classifier model instance eg. kneighbhorClf(),                                  LogisticReg() or others. The model which is to                                  be trained or to which the given data is to be fit.                                  The instance should be created before calling fit! to                                  train it.\nùêó::‚ÑçVector               :- Vector of Hermitian matrices or simply a HermitianVector.                                 The vector of points in the training set consisting of                                 Symmetric Positive Definite manifold matrices.\ny :: Int[]               :- Vector of intrger labels corresponding to each sample in the                                 training set.\nw::Vector(optional) :- Vector containing weights corresponding to every point                                 in ùêó.\n‚úìw = true(optional)      :- Boolean to determine whether to calculate weighted mean                                 or just take w = [].\n\nA value is returned only in case the model is an mdm object. The return value in that case:\n\nclass_means              :- List of means corresponding to all the classes for the                                   given training set.\n\n## Example\nmodel1 = kneighborClf(n_neighbors=3)\nmodel2 = LogisticReg()\nmodel3 = MDM(Fisher)\nn=10\nk1=25\nk2=25\nk=k1+k2\nA1=randP(n)\nA2=randP(n)\nP=randP(n, k1+k2)\ngm=0.5\nP2=[geodesic(Fisher, P[i], A1, gm) for i=1:k1]\nQ2=[geodesic(Fisher, P[i], A2, gm) for i=k1+1:k1+k2 ]\nùêó=‚ÑçVector([P2; Q2])\nY = ones(Int64, 50)\nfor j = 25:50\n    Y[j] = 2\nend\nfit!(model1, ùêó,Y)\nfit!(model2, ùêó,Y)\nfit!(model3, ùêó,Y)\n\n\n\n\n\n","category":"function"},{"location":"train_test/#predict-1","page":"train_test.jl","title":"predict","text":"","category":"section"},{"location":"train_test/#","page":"train_test.jl","title":"train_test.jl","text":"predict","category":"page"},{"location":"train_test/#ScikitLearnBase.predict","page":"train_test.jl","title":"ScikitLearnBase.predict","text":"This function is an overwriting of the default predict function available in the ScikitLearn.jl package. It works similar to the above fit! function. Depending on the type of model i.e. mdm or rest, two different paths are opted. If its mdm, then the predictmdm function from the mdm.jl unit is called that helps in making the prediction. If its not of type mdm, the function just calls the internal function _transformts to make the transformation into the tangent space. This tangent space behaves like an eucledian space. So, now the default predict of ScikitLearn.jl can directly be put to use.\n\nArguments taken by this function are:\n\nmodel::RiemannianML object:- Classifier model instance eg. kneighbhorClf(),                                      LogisticReg() or others. The model which is already                                      been trained according to a training set can only be                                      used as an argument here. The instance should be                                      created and fit before calling predict on it.\nsamp::‚ÑçVector             :- The vector of Hermitian matrices or points in                                      the positive definite manifold for which the                                      prediction is to be made using the model(argument 1)                                      already been trained specially for it.\n\nReturn value:\n\nPredicted classes        :- The List of the predicted classes for the given                                   sample set.\n\n## Example\n# following the above code for fit!\npredict(model1, ùêó)\npredict(model2, ùêó)\npredict(model3, ùêó)\n\n\n\n\n\n","category":"function"},{"location":"train_test/#cross*val*score-1","page":"train_test.jl","title":"crossvalscore","text":"","category":"section"},{"location":"train_test/#","page":"train_test.jl","title":"train_test.jl","text":"cross_val_score","category":"page"},{"location":"train_test/#ScikitLearn.Skcore.cross_val_score","page":"train_test.jl","title":"ScikitLearn.Skcore.cross_val_score","text":"This function is an overwriting of the default crossvalscore function available in the ScikitLearn.jl package. It works similar to the above mentioned fit! and predict functions. Depending on the type of model i.e. mdm or rest, two different paths are opted. If its mdm, then the crossvalmdm function from the crossmdm.jl unit is called that helps in doing cross-validation evaluation. If its not of type mdm, the function just calls the internal function _transformts to make the transformation into the tangent space. This tangent space behaves like an eucledian space. So, now the default crossvalscore of ScikitLearn.jl can directly be put to use.\n\nArguments taken by this function are:\n\nmodel::RiemannianML object:- Classifier model instance eg. kneighbhorClf(),                                      LogisticReg() or others i,e. the model whose                                      evaluation using cross-validation is to be done.\nùêó::‚ÑçVector                :- Vector of Hermitian matrices or simply a HermitianVector.                                      The vector of points in the training set consisting of                                      Symmetric Positive Definite manifold matrices. The                                      training set on the basis of which the evaluation                                      of the given model is to be done.\ny :: Int[]                :- Vector of intrger labels corresponding to each sample in the                                      training set.\ncv :: Int(optional)       :- The number of cross-validation desired by the user.                                      The default value is set to 5.\n\nReturn value:\n\ncross_val score          :- The list containing the score for each cross_val                                   iteration.\n\n   ## Example\n   model1 = kneighborClf(n_neighbors=3)\n   model2 = LogisticReg()\n   model3 = MDM(Fisher)\n   n=10\n   k1=25\n   k2=25\n   k=k1+k2\n   A1=randP(n)\n   A2=randP(n)\n   P=randP(n, k1+k2)\n   gm=0.5\n   P2=[geodesic(Fisher, P[i], A1, gm) for i=1:k1]\n   Q2=[geodesic(Fisher, P[i], A2, gm) for i=k1+1:k1+k2 ]\n   ùêó=‚ÑçVector([P2; Q2])\n   Y = ones(Int64, 50)\n   for j = 25:50\n       Y[j] = 2\n   end\n   println(cross_val_score(model1,ùêó,Y))\n   println(cross_val_score(model2,ùêó,Y))\n   cross_val_score(model5, ùêó,Y)\n\n\n\n\n\n","category":"function"},{"location":"mdm/#mdm.jl-1","page":"mdm.jl","title":"mdm.jl","text":"","category":"section"},{"location":"mdm/#","page":"mdm.jl","title":"mdm.jl","text":"This unit implemets the MDM (Minimum Distance to Mean) classifier for the manifold of positive definite matrices. Similarly to what is done in ScikitLearn in Python, a type is created (struct in Julia) of the desired specifications. Module incorporates supporting functions :   finddist,   predictmdm,   predict_prob.","category":"page"},{"location":"mdm/#","page":"mdm.jl","title":"mdm.jl","text":"It implemens a structure MDM and includes the following functions :","category":"page"},{"location":"mdm/#","page":"mdm.jl","title":"mdm.jl","text":"Function Description\nmean_mdm calculates the mean of all the classes in the training set and also    \t\t \t\tnotifies the user if the mean is not convergent in case of some metric \t\t\t\tspaces\nfind_dist finds the distance of each sample case from the so found means of all the classes\npredict_mdm predicts the class for each sample case depending on its distance from the respective means\npredict_prob predicts the probability of each class for each sample case depending on its distance from the respective means","category":"page"},{"location":"mdm/#","page":"mdm.jl","title":"mdm.jl","text":"For a detailed understanding of mdm, one should know the basics of Riemannian Geometry and its application in the classification of positive definite matrices. One may refer to the following papers for getting the feel of the process.","category":"page"},{"location":"mdm/#","page":"mdm.jl","title":"mdm.jl","text":"A. Barachant, S. Bonnet, M. Congedo, C. Jutten (2012) Multi-class Brain Computer Interface Classification by Riemannian Geometry, IEEE Transactions on Biomedical Engineering, 59(4), 920-928.","category":"page"},{"location":"mdm/#","page":"mdm.jl","title":"mdm.jl","text":"A. Barachant, S. Bonnet, M. Congedo, C. Jutten (2013) Classification of covariance matrices using a Riemannian-based kernel for BCI applications, Neurocomputing, 112, 172-178.","category":"page"},{"location":"mdm/#","page":"mdm.jl","title":"mdm.jl","text":"M. Congedo, A. Barachant, R. Bhatia R (2017a) Riemannian Geometry for EEG-based Brain-Computer Interfaces; a Primer and a Review, Brain-Computer Interfaces, 4(3), 155-174.","category":"page"},{"location":"mdm/#","page":"mdm.jl","title":"mdm.jl","text":"M. Congedo, A. Barachant, E. Kharati Koopaei (2017b) Fixed Point Algorithms for Estimating Power Means of Positive Definite Matrices, IEEE Transactions on Signal Processing, 65(9), 2211-2220.","category":"page"},{"location":"mdm/#","page":"mdm.jl","title":"mdm.jl","text":"Or one may directly look into the Intro to Riemannian Geometry section of the PostDefManifold documentation and quench all their doubts. ","category":"page"},{"location":"mdm/#MDM-structure-1","page":"mdm.jl","title":"MDM structure","text":"","category":"section"},{"location":"mdm/#","page":"mdm.jl","title":"mdm.jl","text":"MDM","category":"page"},{"location":"mdm/#RiemannianML.MDM","page":"mdm.jl","title":"RiemannianML.MDM","text":"This is a structure of the MDM model. It has two attributes namely metric and class_means.\n\nmetric :: Metric        :- The user needs to specify the metric space in which all the distance computations is to be done. The possible options are:\n\nMetric Mean estimation\nEuclidean distance: Œ¥_e; mean: Arithmetic\ninvEuclidean distance: Œ¥_i; mean: Harmonic\nChoEuclidean distance: Œ¥_c; mean: Cholesky Euclidean\nlogEuclidean distance: Œ¥_l; mean: Log Euclidean\nlogCholesky distance: Œ¥_c; mean: Log-Cholesky\nFisher distance: Œ¥_f; mean: Fisher (Cartan, Karcher, Pusz-Woronowicz,...)\nlogdet0 distance: Œ¥_s; mean: LogDet (S, Œ±, Bhattacharyya, Jensen,...)\nJeffrey distance: Œ¥_j; mean: Jeffrey (symmetrized Kullback-Leibler)\nVonNeumann distance: Œ¥_v; mean: Not Availale\nWasserstein distance: Œ¥_w; mean: Wasserstein (Bures, Hellinger, ...)\n\nclass_means             :- This is not to be specified by the user. This comes to play when                               a model is fit with a set of training data so we have to store the means corresponding                               to each of the classes. We can directly access this for an already fit model                               while using functions like predict.\n\n\n\n\n\n","category":"type"},{"location":"mdm/#mean_mdm-1","page":"mdm.jl","title":"mean_mdm","text":"","category":"section"},{"location":"mdm/#","page":"mdm.jl","title":"mdm.jl","text":"mean_mdm","category":"page"},{"location":"mdm/#RiemannianML.mean_mdm","page":"mdm.jl","title":"RiemannianML.mean_mdm","text":"This function is kind of an interface to the mean function of PostDefManifold for computing means for mdm classifier whith metrics other than Fisher, logdet0 and Wasserstein. For these three types of metrics specially it is an interface to their respective mean functions in PostDefManifold. Since the above three metrics do not have any closed form solution for estimating the mean, hence we follow iterative algorithms to compensate for that. As iterative algorithms are always accompanied by the question of convergence, so, in order to answer that we need a convergence check for these three metric types which is facilitated by their respective mean functions. Thereby calling them instead of the generalised mean function specifically for these three kinds of metric.\n\nArguments taken by this function are:\n\nmetric :: Metric        :-The user needs to specify the metric space in which                               all the distance computations is to be done.\nùêè::‚ÑçVector              :-Vector of Hermitian matrices or simply a HermitianVector.                                    The vector of points in the Symmetric Positive Definite                                    manifold to be transformed into the the tangent space.\nw::Vector(optional):- Vector containing weights corresponding to every point in ùêè.\n‚úìw = true(optional)    :- Boolean to determine whether to calculate weighted mean or just take w = [].\n‚è© = false (optional)   :- Boolean to allow threading or not.\n\nReturn value :\n\nG :: ‚Ñç                  :- Mean of the set of ‚ÑçVector.\n\n\n\n\n\n","category":"function"},{"location":"mdm/#find_dist-1","page":"mdm.jl","title":"find_dist","text":"","category":"section"},{"location":"mdm/#","page":"mdm.jl","title":"mdm.jl","text":"find_dist","category":"page"},{"location":"mdm/#RiemannianML.find_dist","page":"mdm.jl","title":"RiemannianML.find_dist","text":"This is a function to find the distance of each sample case from the so found means of all the classes. Distance is caluclated in the metric space opted by the user while creating the instance of mdm.\n\nArguments taken by this function are:\n\nsample :: ‚ÑçVector       :-The vector of Hermitian matrices or points in                                      the positive definite manifold for which the                                      prediction is to be made using the model(argument 1)                                      already been trained specially for it.\nclass_means             :- Vector of Hermitian matrices that represent the class                               means or the centroid of all the classes in the training set.\nmetric :: Metric        :-The user needs to specify the metric space in which                               all the distance computations is to be done.\n\nReturn value:\n\nA :: Array{Float64,2}   :- Array of distances of each of the sample case from                                   each of the class_means.\n\n\n\n\n\n","category":"function"},{"location":"mdm/#predict_mdm-1","page":"mdm.jl","title":"predict_mdm","text":"","category":"section"},{"location":"mdm/#","page":"mdm.jl","title":"mdm.jl","text":"predict_mdm","category":"page"},{"location":"mdm/#RiemannianML.predict_mdm","page":"mdm.jl","title":"RiemannianML.predict_mdm","text":"This is a function to predict the class for each sample case depending on its distance from the respective means. The class associated with the closest mean of the sample is alloted to that sample case. So the name comes Minimum distance to Mean as the mean which is at the minimum distance to the sample decides the class of the sample.\n\nArguments taken by this function are:\n\nsample :: ‚ÑçVector       :- The vector of Hermitian matrices or points in                                          the positive definite manifold for which the                                          prediction is to be made using the model(argument 1)                                          already been trained specially for it.\nclass_means             :- Vector of Hermitian matrices that represent the class                                   means or the centroid of all the classes in the training set.\nmetric :: Metric        :- The user needs to specify the metric space in which                               all the distance computations is to be done.\n\nReturn value:\n\nresult :: Array{Int, 1} :- The List of the predicted classes for the given                                   sample set.\n\n\n\n\n\n","category":"function"},{"location":"mdm/#predict_prob-1","page":"mdm.jl","title":"predict_prob","text":"","category":"section"},{"location":"mdm/#","page":"mdm.jl","title":"mdm.jl","text":"predict_prob","category":"page"},{"location":"mdm/#RiemannianML.predict_prob","page":"mdm.jl","title":"RiemannianML.predict_prob","text":"This is a function to predict the probability of each class for each sample case depending on its distance from the respective means. The class associated with the closest mean is having the highest probability. All the probability sums to 1. This function makes use of the softmax function from thr PostDefManifold to calculate the probability values.\n\nArguments taken by this function are:\n\nsample :: ‚ÑçVector       :-The vector of Hermitian matrices or points in                                          the positive definite manifold for which the probability                                          prediction is to be made using the model(argument 1)                                          already been trained specially for it.\nclass_means             :- Vector of Hermitian matrices that represent the class                                   means or the centroid of all the classes in the training set.\nmetric :: Metric        :-The user needs to specify the metric space in which                               all the distance computations is to be done.\n\nReturn value:\n\nProb :: Array{Float64,1}:- The List of the predicted probabilities corresponding                                   to each of the classes for the given sample set.\n\n\n\n\n\n","category":"function"},{"location":"cross_mdm/#cross_mdm.jl-1","page":"cross_mdm.jl","title":"cross_mdm.jl","text":"","category":"section"},{"location":"cross_mdm/#","page":"cross_mdm.jl","title":"cross_mdm.jl","text":"This unit contains cross validation algorithm for MDM( Minimum Distance to Mean) classifier i.e. applied directly on Positive Definite Manifold. Similar to ScikitLearn in Python, one can have a better evaluation of the classifier by using cross validation. Unit contains supporting function indCV and function crossvalmdm.","category":"page"},{"location":"cross_mdm/#","page":"cross_mdm.jl","title":"cross_mdm.jl","text":"Function Description\nindCV returns the vectors containing shuffled indices of training and testing samples for each cross validation iteration\ncross_val_mdm implements cross validation for MDM classifier","category":"page"},{"location":"cross_mdm/#indCV-1","page":"cross_mdm.jl","title":"indCV","text":"","category":"section"},{"location":"cross_mdm/#","page":"cross_mdm.jl","title":"cross_mdm.jl","text":"indCV","category":"page"},{"location":"cross_mdm/#RiemannianML.indCV","page":"cross_mdm.jl","title":"RiemannianML.indCV","text":"This is a helper function to implement crossvalmdm. It returns the vectors containing indices of training and testing samples for each cross validation iteration. The indices so received are shuffled well and not contiguous strings or blocks of data samples. This function uses shuffle! to shuffle all the indices and then divides the shuffled set into training and testing sets. This function holds the prime basis of CrossValidation implementation.\n\nArguments taken by this function are:\n\nk::Int                 :- Last number of the sequence of natural numbers to be                                   shuffled starting from 1\nnCV:: Int              :- Number of cross-validation for which the indices are                                   to be generated.\nshuffle                :- Boolean to inform whether to do shuffling or not.                                Default is set to True.\n\nReturn values :\n\nnTest                  :- The size of each testing set.\nnTrain                 :- The size of each training set.\nindTrain               :- The list of all the vectors that contain the training                                   indices for each iteration.\nindTest                :- The list of all the vectors that contain the testing                                   indices for each iteration.\n\n\n\n\n\n","category":"function"},{"location":"cross_mdm/#cross*val*mdm-1","page":"cross_mdm.jl","title":"crossvalmdm","text":"","category":"section"},{"location":"cross_mdm/#","page":"cross_mdm.jl","title":"cross_mdm.jl","text":"cross_val_mdm","category":"page"},{"location":"cross_mdm/#RiemannianML.cross_val_mdm","page":"cross_mdm.jl","title":"RiemannianML.cross_val_mdm","text":"This function performs cv number of iterations in each of which it runs the mdm model over a part of the shuffled data. This way helps us to have a better evaluation of our classifier.\n\nThis is the main function implementing cross validation for MDM classifier. It uses indCV as its basic helper function. It returns the vector of confusion matrices, coreesponding to each cross valdation iteration and also the final confusion matrix i.e. the sum of all the previous ones. The function also returns average balanced accuracy or average regular accuracy depending on the choice of the user. The default is set to balanced accuracy type. It takes the following arguments:\n\nArguments taken by this function are:\n\nùêó :: ‚ÑçVector          :- The training set of type Vector of Hermitian matrices.\ny                      :- The list of labels corresponding to each trial\ncv :: Int              :- The number of cross-validation desired by the user\nscoring :: String      :- If Balanced Accuracy is requied or the Regular Accuracy.                                   The default is set to Balanced Accuracy.\nmetric :: Metric       :- The metric space in which to do the computations. To be                                   specified by the user as one of the many metric spaces options provided in PostDefManifold.                                   One may refer to [mdm.jl documentations] for exploring all the possible options.                                   The default is set to Fisher.\n\nReturn value:\n\ncross_val score       :- The list of the balanced accuracy score for each                               cross_val iteration.\n\n\n\n\n\n","category":"function"},{"location":"example/#example.jl-1","page":"example.jl","title":"example.jl","text":"","category":"section"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"This unit demonstrates the use of RiemannianML using various examples. It can be used as a referrence guide while working in RiemannianML. Examples are provided for all the functions and structure objects. The corresponding unit example.jl contains the code corresponding to each and every example citation, one may directly copy these codes from there and run on their own to check its working. The results cited here might be different then the ones obtained by the user because random points generation is done to create stimulated data, so it may vary every time but the overall range varies roughly over the same stretch. ","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"For these examples below, simulated data are used. These data are created by few simple lines of codes. Let us assume that the number of eclectrodes in our situation is 30.","category":"page"},{"location":"example/#Simulated-data-creation-(not-necessary)-1","page":"example.jl","title":"Simulated data creation (not necessary)","text":"","category":"section"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"So, we fix n = 30. This implies our data will consist of 30 x 30 Hermitian matrices. For the real cases these simply be Real matrices.","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"n=30","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"In the following examples, binary classification is demonstrated. Let the training examples for each of the two classes be 80. So, k1 = 80 and k2 = 80. The total training set size =k1 + k2","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"k1=80\r\nk2=80\r\nk=k1+k2","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"To create stimulated data for EEG, we randomly pick two points A1, A2 in the Positive Definite manifold. Let A1 and A2 be the standard cases representing class 1 and class 2 respectively.","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"A1=randP(n)\r\nA2=randP(n)","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"For creating the entire training set, that could behave as the training samples for classes 1 and 2. These should be similar somewhat to either of A1 or A2. To ensure this, first of all a set containing k1 + k2 random points in the post def manifold is generated.","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"P=randP(n, k1+k2)","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"Now, we move each of these points closer to either A1 or A2 so they have some resemblence to them. This will ensure that our data is worth classifying. We move these points slowly and slowly closer to the standards chosen( A1, A2 ). This closeness is monitored by the value gm here. gm = 0 means, no shifting is done, they are just random set of points. gm = 1 means, all the points are shifted exactly to A1 or A2. All the intermediate values of gm will take the points to intermediate distance ratios on their geodesic joining A1/A2. So, half of the points are taken closer to A1, and the other half to A2. This means half are closer to class 1 while the other half to class 2.","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"gm=0.1\r\nP2=[geodesic(Fisher, P[i], A1, gm) for i=1:k1]\r\nQ2=[geodesic(Fisher, P[i], A2, gm) for i=k1+1:k1+k2 ]\r\nùêó=‚ÑçVector([P2; Q2])","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"The new shifted set of points are contained in P2 and Q2. We concatenate them into one set ùêó, which represent now the simulated training set. Then the label is created for this training set.","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"Y = [repeat([1], k1); repeat([2],k2)]","category":"page"},{"location":"example/#Model-declaration-1","page":"example.jl","title":"Model declaration","text":"","category":"section"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"Like in ScikitLearn, for applying a model on your data, first of all create an instance or object of the corresponding model class with all specifications. Here they are not classes but structures.","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"Model instance for kneighborClf that does KNeighborsClassifier classification.","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"model1 = kneighborClf(n_neighbors=3)","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"Model instance for LogisticReg that applies LogisticRegression.","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"model2 = LogisticReg()","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"Model instance for LinearSVM that applies LinearSVC.","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"model3 = LinearSVM()","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"Model instance for SVM that applies SVC.","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"model4 = SVM()","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"Model instance for MDM( Minimum Distance to Mean ) that applies mdm classification.","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"model5 = MDM(Fisher)","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"Model instance for LogisticRegCV that applies LogisticRegressionCV( CV - cross-validation ).","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"model6 = LogisticRegCV()","category":"page"},{"location":"example/#Fitting-to-the-model-1","page":"example.jl","title":"Fitting to the model","text":"","category":"section"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"The so created simulated training sets are then fit to the models. This is done simply by calling the fit! function that takes 3 arguments, model, training set and labels. fit! is the first function i.e. to be called so that our model is ready to make predictions.","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"fit!(model1, ùêó,Y)\r\nfit!(model2, ùêó,Y)\r\nfit!(model5, ùêó,Y)\r\nfit!(model3, ùêó,Y)\r\nfit!(model4, ùêó,Y)\r\nfit!(model6, ùêó,Y)","category":"page"},{"location":"example/#Cross-Validation-1","page":"example.jl","title":"Cross-Validation","text":"","category":"section"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"In order to evaluate the model performance, cross-validation is done. The score of cross-validation is returned by the crossvalscore function which is then printed here.","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"println(cross_val_score(model1,ùêó,Y, cv = cv_fold))\r\nprintln(cross_val_score(model2,ùêó,Y, cv = cv_fold))\r\nprintln(cross_val_score(model5,ùêó,Y, cv = cv_fold))","category":"page"},{"location":"example/#Making-prediction-using-the-predict-function-1","page":"example.jl","title":"Making prediction using the predict function","text":"","category":"section"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"Now, in order to make the predictions, predict function is to be employed. Here also, a simulated sample set is feeded into the predict function to check if the prediction is made correctly or not. Again the same procedure as the one used for training set generation, is followed. After doing the desired amount of shifting, the points are stored in samp.","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"T = randP(n,5)\r\nS = randP(n,3)\r\ntm = 0.198\r\nT1 = [geodesic(Fisher, T[i], A1, tm) for i=1:length(T)]\r\nS1 = [geodesic(Fisher, S[i], A2, tm) for i=1:length(S)]\r\nsamp = ‚ÑçVector([T1; S1])","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"Calling the predict function on the testing sample samp with different fit models.","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"predict(model2, samp)\r\npredict(model5, samp)","category":"page"},{"location":"example/#Table-representing-performance-score-for-all-the-models.-1","page":"example.jl","title":"Table representing performance score for all the models.","text":"","category":"section"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"A table is constructed that holds the cross-validation score for each of the models corresponding to different training sets. These different training sets are simulated training sets for an increasing value of gm i.e. extent of shifting. For this all the models are put in the list model. The following for loop fills values into this table. The table is printed.","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"model = [model1, model2, model3, model4, model5, model6]\r\nprintln(model)\r\ncv_fold = 4\r\ntable = Array{Float64, 2}(undef, 6,9)\r\nfor i = 1:6\r\n    for j = 1:9\r\n        gm = 0.035 * j\r\n        P2=[geodesic(Fisher, P[i], A1, gm) for i=1:k1]\r\n        Q2=[geodesic(Fisher, P[i], A2, gm) for i=k1+1:k1+k2 ]\r\n        ùêó=‚ÑçVector([P2; Q2])\r\n        #fit!(model[i], ùêó,Y )\r\n        table[i,j] = (ùö∫(cross_val_score(model[i],ùêó,Y, cv = cv_fold)))/ cv_fold\r\n    end\r\nend\r\n\r\nprintln(table)\r\n\n\n\n>> [0.493827 0.512346 0.556268 0.625594 0.719611 0.825024 0.90622 0.974834 0.993827; \r\n0.650285 0.937559 0.987654 0.993827 0.993827 1.0 1.0 1.0 1.0; \r\n1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; \r\n0.524454 0.54321 0.655508 0.755223 0.855413 0.981007 0.993827 0.993827 1.0; \r\n0.59375 0.55 0.6 0.7875 0.8625 0.975 0.9875 0.99375 1.0; \r\n1.0 1.0 1.0 1.0 1.0 0.993827 1.0 1.0 1.0]","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"Where as for a value of gm = 0.015 * [1:9], we get the following set of values. Values are tabulated.","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"Model\\gm 0.015 0.030 0.045 0.060 0.075 0.900 0.105 0.120 0.135\nkneighbor 0.49375 0.4875 0.49375 0.5125 0.5125 0.53125 0.5375 0.575 0.625\nLogisticReg 0.54375 0.7 0.83125 0.93125 0.98125 0.99375 0.99375 0.99375 0.99375\nLinearSVM 0.89375 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\nSVM 0.53125 0.53125 0.55625 0.56875 0.59375 0.61875 0.6625 0.68125 0.71875\nMDM 0.49375 0.53125 0.5375 0.5875 0.6625 0.65625 0.63125 0.65625 0.6875\nLogisticRegCV 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"For a value of gm = 0.05 * [1:9], we get the following set of values. Values are tabulated.","category":"page"},{"location":"example/#","page":"example.jl","title":"example.jl","text":"Model\\gm 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45\nkneighbor 0.49375 0.5375 0.65 0.80625 0.91875 0.9875 1.0 1.0 1.0\nLogisticReg 0.86875 0.99375 0.99375 1.0 1.0 1.0 1.0 1.0 1.0\nLinearSVM 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\nSVM 0.5625 0.65625 0.79375 0.96875 0.99375 1.0 1.0 1.0 1.0\nMDM 0.5875 0.64375 0.81875 0.9875 0.99375 1.0 1.0 1.0 1.0\nLogisticRegCV 1.0 1.0 1.0 1.0 0.99375 1.0 1.0 1.0 1.0","category":"page"}]
}
